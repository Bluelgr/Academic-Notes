\documentclass{article}

\usepackage{amsmath,mathtools,amssymb}
\usepackage{bm,extarrows,ulem,cancel}
\usepackage{mathrsfs}
\usepackage{geometry,graphicx,color}
\geometry{centering,scale=0.8}

\newcommand{\sect}{\section}
\newcommand{\subsec}{\subsection}

\newcommand{\be}{\begin{equation}}
\newcommand{\ee}{\end{equation}}
\newcommand{\bea}{\begin{eqnarray}}
\newcommand{\eea}{\end{eqnarray}}
\newcommand{\ba}{\begin{array}}
\newcommand{\ea}{\end{array}}
\newcommand{\bs}{\be\begin{split}}

\newcommand{\dif}{\,\mathrm{d}}
\newcommand{\p}{\partial}
\newcommand{\1}{\left}
\newcommand{\2}{\right}
\newcommand{\ma}{\mathcal}
\newcommand{\la}{\langle}
\newcommand{\ra}{\rangle}

\newcommand{\m}{\mu}
\newcommand{\n}{\nu}
\newcommand{\al}{\alpha}
\newcommand{\bet}{\beta}
\newcommand{\lam}{\lambda}
\newcommand{\sig}{\sigma}
\newcommand{\ep}{\epsilon}
\newcommand{\om}{\omega}
\newcommand{\del}{\delta}
\newcommand{\Del}{\Delta}


\title{Notes on The Theoretical Minimum\\
--- Quantum Mechanics}
\author{Gui-Rong Liang}

\begin{document}
\maketitle
\tableofcontents

\newpage

\section{Mathematical Foundations and Spin systems}
\subsection{Basic concepts and fundamental principles}
1. Quantum states, operators, Hermitian properties\\

A quantum state is represented by a ket vector that can be decomposed using the orthogonal basis vectors, in this subsection we mainly focus on discrete basis, thus 
\be
|A\ra=\sum_j \al_j|j\ra, \quad\text{with} \quad \al_j=\la j|A\ra,
\ee
where the coefficient $\al_j$ is a complex number, and is also called a wave function. Thus
\be
|A\ra=\sum_j |j\ra \la j|A\ra \implies  \sum_j |j\ra \la j|=1.
\ee
The probability of measuring a state corresponding to a certain basis is given by
\be
P_j=\al_j^*\al_j=\la A |j\ra \la j|A\ra,
\ee
then the normalization of probability gives to 
\be
1=\sum_j P_j=\sum_j\la A |j\ra \la j|A\ra=\la A|A\ra.
\ee
So the general principle is the state of a system is represented by a unit vector.\\

Now we introduce the matrix representation of an operator, which turn a vector to another vector,
\be
M|A\ra=|B\ra.
\ee
If we decompose A and B into components on basis vectors, we can represent the above equation as,
\be
\sum_j M |j\ra \al_j=\sum_j \bet_j|j\ra,
\ee
multiplying the above by a basis bra, we have
\be
\sum_j \la k| M |j\ra \al_j=\sum_j \bet_j \la k|j\ra = \bet_k,
\ee
which is denoted by 
\be
\sum_j M_{kj} \al_j = \bet_k,
\ee
where $M_{ij}\equiv \la k|M|j\ra$ is matrix element of the operator.\\
We write the above as explicit matrix form (taking 2 dimension as an example) as
\be
\1(\ba{cc}M_{11}&M_{12}\\M_{21}&M_{22}\ea\2)\1(\ba{c}\al_1\\\al_2\ea\2)=\1(\ba{c}\bet_1\\\bet_2\ea\2).
\ee
Now we take its complex conjugate and make it a equation of row vectors,
\be
\1(\ba{cc}\al_1^*&\al_2^*\ea\2)\1(\ba{cc}M_{11}^*&M_{21}^*\\M_{12}^*&M_{22}^*\ea\2)=\1(\ba{cc}\bet^*_1&\bet^*_2\ea\2),
\ee
which can be represented by the bras notation,
\be
\la A|M^\dagger =\la B|,
\ee
where $M^\dagger$ is called the \textit{Hermitian conjugate} of $M$, with the elements of $M^\dagger$ is given by
\be
(M^\dagger)_{jk}=M^*_{kj}.
\ee
An Hermitian operator is an operator which equals to its own Hermitian conjugate,
\be
M=M^\dagger.
\ee
Physical observable quantities are represented by Hermitian operators, due to the fact that the eigenvalues of Hermitian operators are real. And further, eigenvectors of an Hermitian operator corresponding to different eigenvalues are orthogonal. Next we will prove this.\\
If
\be
L|\lam\ra=\lam|\lam\ra,
\ee
where $L$ is Hermitian, we have
\be
\lam^*\la\lam|\lam\ra=\la\lam|L^\dagger|\lam\ra=\la\lam|L|\lam\ra=\lam\la\lam|\lam\ra \implies \lam^*=\lam,
\ee
Thus $\lam$ is real. Then if
\be\1\{\begin{split}
L|\lam_1\ra&=\lam_1|\lam_1\ra \\
L|\lam_2\ra&=\lam_2|\lam_2\ra,
\end{split}\2.\ee
we have
\be
\lam_1\la\lam_1|\lam_2\ra=\la\lam_1|L|\lam_2\ra=\lam_2\la\lam_1|\lam_2\ra \implies (\lam_1-\lam_2)\la\lam_1|\lam_2\ra=0 \implies \la\lam_1|\lam_2\ra=0.
\ee
Even if eigenvectors of the same eigenvalue are not orthogonal, we can make it orthogonal by Gram-Schmidt procedure. Finally we can make all eigenvectors of an Hermitian operator orthogonal, thus they can be used as a basis for the vector space.\\

Another inference is that if two operators $L$, $M$ has the same set of eigenvectors, they commute. We'll use the labels $\lam_i$ and $\m_\al$ to denote the eigenvalues of $L$ and $M$, and assume that there is a basis of state-vectors $|\lam_i,\m_\al\ra$ that are simultaneous eigenvectors of both observables.
\bs
L|\lam_i,\m_\al\ra&=\lam_i|\lam_i,\m_\al\ra\\
M|\lam_i,\m_\al\ra&=\m_\al|\lam_i,\m_\al\ra,
\end{split}\ee
the by acting on any of the basis vector, which we omit the subscript later, by the two operators,
\be
LM|\lam,\m\ra=\lam\m|\lam,\m\ra=ML|\lam_i,\m_\al\ra \implies [L,M]|\lam,\m\ra=0,
\ee
since any vector is composed of the basis vectors, $[L,M]$ acting on any vector gives zero,
\be [L,M]=0,\ee
thus they commute. It turns out that he converse of this theorem is also true: if two observables commute, then there is a complete basis of simultaneous eigenvectors of the two observables. Moreover, one may need to specify a larger number of observables to completely label a basis, regardless of the number of observables that are needed, they must all commute, we call this collection \textit{a complete set of commuting observables}.\\

2. The Uncertainty Principle\\

Suppose the eigenvalues of the observable $A$ is called $a$, then the expectation value of $A$ is
\be
\la A\ra= \sum_a aP(a).
\ee
Roughly speaking this means that $P(a)$ is centered around the expectation value. What we will mean by "the uncertainty in $A$ is the so-called \textit{standard deviation} $\bar A$, which is computed by subtracting from $A$ its expectation value,
\be
\bar A=A-\la A\ra=A-\la A\ra I.
\ee
The eigenvectors of $\bar A$ are the same as those of $A$ and the eigenvalues of $\bar A$ is shifted from that of $A$ by the average value $\la A\ra$,
\be
\bar A=a-\la A\ra.  
\ee
The square if the uncertainty of $A$ is defined by
\be
(\Del A)^2:=\sum_a \bar A^2 P(a)=\sum_a (a-\la A\ra)^2 P(a)=\la \Psi| \bar A^2|\Psi\ra,
\ee
if the expectation value of $A$ is zero, then the uncertainty takes the simpler form
\be
(\Del A)^2=\la \Psi| A^2|\Psi\ra.
\ee

If $\vec X$ and $\vec Y$ are two vectors, then the \textit{Triangle Inequality} gives
\be
|\vec X|+|\vec Y|\geqslant |\vec X+\vec Y|,
\ee
by squaring and sorting it we have
\be
|\vec X||\vec Y|\geqslant \vec X\cdot\vec Y,
\ee
and by squaring it again we have
\be
|\vec X|^2|\vec Y|^2\geqslant |\vec X\cdot\vec Y|^2,
\ee
which is called the \textit{Cauchy-Schwarz} inequality.

For complex vector spaces, the magnitude of the vectors are given by
\bs
|X|&=\sqrt{\la X|X\ra}\\
|X+Y|&=\sqrt{(\la X|+\la Y|)(|X\ra+|Y\ra)},
\end{split}\ee
squaring them and using the Triangle Inequality we have
\be
2|X||Y|\geqslant |\la X|Y\ra+\la Y|X\ra|.
\ee
Now we take $|X\ra$ and $|Y\ra$ as follows,
\bs
|X\ra&=A|\Psi\ra\\
|Y\ra&=iB|\Psi\ra,
\end{split}\ee
where $A$ and $B$ are any two observables and $|\Psi\ra$ is any ket. Now substitute it into the Triangle Inequality for complex vectors, to get
\be
2\sqrt{\la A^2\ra\la B^2\ra} \geqslant |\la \Psi|AB|\Psi\ra-\la \Psi|BA|\Psi\ra|=\la \Psi|[A,B]|\Psi\ra.
\ee
Let's suppose for the moment that $A$ and $B$ have expectation values of zero, so $A=\bar A$, and $\la A^2\ra=(\Del A)^2$, thus we rewrite the above inequality as 
\be
\Del A\Del B\geqslant \frac 1 2\la \Psi|[A,B]|\Psi\ra.
\ee
This is the general version of the uncertainty inequality. It says that the product of the uncertainties cannot be smaller than half the magnitude of the expectation value of the commutator. Or to be less quantitative, if the commutator of $A$ and $B$ is not zero, then both observables cannot simultaneously be certain. By the way, if the expectation values of $A$ and $B$ are not zero, we just use a trick to redefine two new operators in which the expectation values have been subtracted off, then the same relation holds as we expected.\\

Later we will come to a specific version of the inequality --- \textit{Heisenberg's Uncertainty Principle}: The product of the uncertainties of the position and momentum of a particle cannot be less than half of Plank's constant.


\subsection{Spin states, spin operators, and Pauli Matrices}
\textit{All possible spin states can be represented in a two dimensional vector space.}

\be
|A\ra=\al_u|u\ra+\al_k|d\ra,
\ee
where the $u$ and $d$ represents for pointing up and down when we measure spin along z-axis, and the orthogonality means if we get up we never get down, vice versa.\\
Now we choose A to be pointing left and right, considering that they're orthonormal, and it is equally likely to be up and down when measured along z, we write them as
\be\1\{\begin{split}
|r\ra&=\frac1 {\sqrt{2}}|u\ra+\frac1 {\sqrt{2}}|d\ra\\
|l\ra&=\frac1 {\sqrt{2}}|u\ra- \frac1 {\sqrt{2}}|d\ra.
\end{split}\2.\ee
And when we represent forward and backward, we must further take into account the equal likelihood of left and right when measured along x-axis, so we take the following form,
\be\1\{\begin{split}
|f\ra&=\frac1 {\sqrt{2}}|u\ra+\frac i {\sqrt{2}}|d\ra\\
|b\ra&=\frac1 {\sqrt{2}}|u\ra- \frac i {\sqrt{2}}|d\ra.
\end{split}\2.\ee
Notice that all the above choices have the same freedom by a phase factor $e^{i\theta}$.

We will use $\sig$ to denote spin operator. Measuring along z-axis gives us
\be\1\{\begin{split}
\sig_z |u\ra&=|u\ra\\
\sig_z |d\ra&=-|d\ra,
\end{split}\2.\ee
if we denote $|u\ra$ and $|d\ra$ as unit vectors as $\1(\ba{c}1\\0\ea\2)$ and $\1(\ba{c}0\\1\ea\2)$, we may figure out that
\be
\sig_z=\1(\ba{cc}1&0\\0&-1\ea\2).
\ee
Measuring along x-axis gives us
\be\1\{\begin{split}
\sig_x |r\ra&=|r\ra\\
\sig_x |l\ra&=-|l\ra,
\end{split}\2.\ee
if we denote $|r\ra$ and $|l\ra$ as unit vectors as $\1(\ba{c}\frac1{\sqrt{2}}\\\frac1{\sqrt{2}}\ea\2)$ and $\1(\ba{c}\frac1{\sqrt{2}}\\-\frac1{\sqrt{2}}\ea\2)$, we may figure out that
\be
\sig_x=\1(\ba{cc}0&1\\1&0\ea\2).
\ee
Measuring along y-axis gives us
\be\1\{\begin{split}
\sig_y |f\ra&=|f\ra\\
\sig_y |b\ra&=-|b\ra,
\end{split}\2.\ee
if we denote $|f\ra$ and $|b\ra$ as unit vectors as $\1(\ba{c}\frac1{\sqrt{2}}\\\frac i{\sqrt{2}}\ea\2)$ and $\1(\ba{c}\frac1{\sqrt{2}}\\-\frac i{\sqrt{2}}\ea\2)$, we may figure out that
\be
\sig_y=\1(\ba{cc}0&-i\\i&0\ea\2).
\ee
The matrix representations of $\sig_x$, $\sig_y$ and $\sig_z$ are called the \textit{Pauli Matrices}.\\
It is easy and useful to verify the commutation relations of components of spins,
\be
[\sig_j,\sig_k]=2i\ep_{jkl}\sig_l.
\ee

Now we may treat $\sig_x$, $\sig_y$ and $\sig_z$ as three components of $\sig$, thus $\sig$ can be viewed as a general vector, and it can be projected onto a unit vector $\hat n$ point any direction in space, so as to measure spin along that direction.
\be
\sig_n=\vec\sig\cdot\hat n=\sig_xn_x+\sig_yn_y+\sig_zn_z=\1(\ba{cc}n_z&n_x-in_y\\n_x+in_y&-n_z\ea\2),
\ee
from the fact that
\bs
tr(\sig_n)&=0\\
det(\sig_n)&=-1,
\end{split}\ee
we could immediately figure out that the two eigenvalues of $\sig_n$ are give by $\pm 1$. And we could work out its eigenvectors in a certain configuration, if $\hat n$ lies x-z plane, we may write
\be\1\{\begin{split}
n_z&=\cos\theta\\
n_x&=\sin\theta\\
n_y&=0,
\end{split}\2.\ee
thus 
\be
\sig_n=\1(\ba{cc}\cos\theta&\sin\theta\\\sin\theta&-\cos\theta\ea\2),
\ee
so the eigenvectors correspond to each eigenvalues are given by
\be\1\{\begin{split}
\lam_1=1, &\quad|\lam_1\ra=\1(\ba{c}\cos{\frac\theta 2}\\\sin{\frac\theta 2}\ea\2)\\
\lam_2=-1, &\quad|\lam_1\ra=\1(\ba{c}-\sin{\frac\theta 2}\\\cos{\frac\theta 2}\ea\2).
\end{split}\2.\ee
Now suppose we prepare a $|u\ra$ state, and we measure the probabilities of $+1$ and $-1$ along the $\hat n$ direction, we get
\be\1\{\begin{split}
P(+1)=&\la u|\lam_1\ra\la\lam_1|u\ra=\cos^2{\frac{\theta}{2}}\\
P(-1)=&\la u|\lam_2\ra\la\lam_2|u\ra=\sin^2{\frac{\theta}{2}}
\end{split}\2.\ee
Now we're ready to compute the average value of spin along the $n$ direction,
\be
\la \sig_n\ra=\sum_j \lam_j P(\lam_j)=\cos^2{\frac{\theta}{2}}-\sin^2{\frac{\theta}{2}}=\cos{\theta},
\ee
which agrees perfectly with our expectation.\\

To proceed, we come to an important theorem,\\

\textit{\textbf{The Spin-Polarization Principle:} Any state of a single spin is an eigenvector of some component of the spin.}\\

In other words, given any state
\be
|A\ra=\al_u|u\ra+\al_d|d\ra,
\ee
there exists some direction $\hat n$, such that
\be
\vec\sig\cdot\vec n|A\ra= |A\ra.
\ee
In physics language, we say that the states of a spin are characterized by a \textit{polarization vector}, and along that polarization vector the component of spin is predictably $+1$, assuming of course that you know the state-vector.\\ It follows that the expectation value of the spin along the direction $\hat n$ can be expressed as
\be
\la\sig_n\ra=\la \vec\sig\cdot\vec n\ra=1.
\ee\\

In fact, any observable of a spin is represented by a $2\times2$ Hermitian matrix, and has the form
\be
\1(\ba{cc}r&w\\w^*&r'\ea\2),
\ee
the implication is that it takes exactly four real parameters to specify this observable. And, there is a neat way to write any spin observable in terms of the Pauli matrices, $\sig_x$, $\sig_y$, $\sig_z$, and the unit matrix $I$,
\be S=a\sig_x+b\sig_y+c\sig_z+d I, \ee
where $a$, $b$, $c$ and $d$ are real numbers.

\subsection{States and Operators in Continuous basis}
1. States and the inner product\\

A particle at a fixed position $x_0$ (in one dimension) can be represented by a state vector $|x_0\ra$. In general, a state is a superposition of various position state, since position is continuous variable, we must turn the sum into integral,
\be
|\psi\ra=\int \dif x \ \psi(x)|x\ra,
\ee
where the coefficient, or wave function $\psi(x)$ is computed as
\be
\psi(x)=\la x|\psi\ra,
\ee
substituting the coefficient into the decomposition we have
\be
|\psi\ra=\int \dif x \ |x\ra \la x|\psi\ra,
\ee
from which we find an identity as
\be
I=\int \dif x \ |x\ra \la x|,
\ee
which implies summing over all projections $|x\ra \la x|$, we can recover the original state.\\
Left multiplying the decomposition of state by a bra vector $|x'\ra$, we have
\be
\psi(x')=\la x'|\psi\ra=\int \dif x \ \psi(x)\ \la x'|x\ra,
\ee
from which we can see the inner product between basis is nothing but the Dirac-delta function,
\be
\la x'|x\ra=\delta(x'-x),
\ee
this is the orthonormalization relation of the basis vectors.\\

Using the above relations we may naturally find the expression of inner product of states in the continuous position basis, just by inserting two different sets of the identity relations of $x$ and $x'$,
\be
\la \phi|\psi\ra = \int \dif x \int \dif x' \ \la \phi|x'\ra\la x'|x\ra\la x|\psi\ra=\int \dif x \int \dif x' \phi^*(x') \del(x'-x) \psi(x)=\int \dif x \ \phi^*(x)\psi(x),
\ee
thus the inner product of a state with itself is given by
\be
\la \psi|\psi\ra =\int \dif x \ \psi^*(x)\psi(x),
\ee
this will remind us of the expression of the probability, and due to the continuous property we will denote our corresponding concept as the "probability density", which is given by
\be
P(x):=\psi^*(x)\psi(x),
\ee
and the probability of state in an interval $[a,b]$ is given by
\be
P([a,b])=\int_a^b \dif x\ P(x)=\int_a^b \dif x\ \psi^*(x)\psi(x),
\ee
and because the total probability equal to $1$, our state here is normalized,
\be
\la \psi|\psi\ra =\int \dif x \ \psi^*(x)\psi(x)=\int  \dif x\ P(x)=1.\\
\ee
2. Linear Operators, eigenvectors and eigenvalues\\

An operator is a machine which can turn a given state into a new state, in terms of the continuous basis, if the coefficients (or the wave function) of the new state is specified, the new state would be determined, thus the effect of the operator is made clear.

Let's set an example. We want to define an operator $X$ which can turn a state $|\psi\ra$ in to a new state $|\bar\psi\ra$, now the state $|\psi\ra$ is given, so the definition of $X$ is clear only if we know what the new state $|\bar\psi\ra$ is. By ``the state $|\psi\ra$ is given", we mean in terms of any basis, we know exactly what the coefficients of the state $|\psi\ra$ are, here we choose the continuous position basis, where we know exactly the wave function $\psi(x)$. So the task now shifts from defining $X$ to giving the expression of $\bar\psi(x)$, and we define it to be $\bar\psi(x)=x\psi(x)$, thus we say we complete the definition of the operator $X$.

By mathematical expressions, we write the definition of $X$ as
\be
X|\psi\ra=|\bar\psi\ra \quad\text{s.t.}\quad \bar\psi(x)=x\psi(x),
\ee
or in a more neat form,
\be
\la x|X|\psi\ra:=x\la x|\psi\ra.
\ee
Here we say that $x$ is a \textit{basis-dependent operator corresponding to $X$}. Note that an operator only acts on a state in the abstract vector space, while a basis-dependent operator acts on the wave function. Due to the fact that both the abstract state vector and the wave function can be multiplied by a number, sometimes we blur the distinction of the two concept and just write $X=x$, but this is not generally true. The clear relation between $X$ and $x$ can be seen by inserting an identity relation to the definition, as
\be
\int \dif x'\ X_{xx'} \psi(x')=\int \dif x' \ \la x|X|x'\ra\la x'|\psi\ra=\la x|X|\psi\ra=x\la x|\psi\ra=x \psi(x),
\ee
and by observing we have 
\be\1\{\begin{split}
&X_{xx'}= \la x|X|x'\ra=x\del(x-x')\\
&x=\int \dif x'\ X_{xx'}=\int \dif x'\ \la x|X|x'\ra,
\end{split}\2.\ee
which clearly shows in position basis $X$ is a matrix with only non-zero diagonalized elements $x\del(x-x')$, and $x$ is the sum of the elements of the $x'$th column.\\

Then by bearing the above in mind, we proceed to define a new operator $D$ as
\be
\la x|D|\psi\ra:=\frac\p{\p x}\la x|\psi\ra,
\ee
where the partial derivative $\frac\p{\p x}$ is the basis-dependent operator corresponding to $D$. Notice that now we CANNOT write $D=\frac\p{\p x}$, because $\frac\p{\p x}|\psi\ra$ is meaningless due to the basis-independence of $|\psi\ra$. The relation between $D$ and $\frac\p{\p x}$ can be seen by the similar process as in the case of $X$ and $x$, and here we directly list the results:
\be\1\{\begin{split}
D_{xx'}&= \la x|D|x'\ra=\del(x-x')\frac\p{\p x}\\
\frac\p{\p x}&=\int \dif x'\ D_{xx'}=\int \dif x'\ \la x|D|x'\ra.
\end{split}\2.\ee\\

Operators with physical interests are Hermitian operators. A convenient way to judge the hermitian property is by using the inner product. The inner product is number, assuming $O$ is hermitian, we take the complex conjugate of the inner product,
\be
\la\phi|O|\psi\ra^*=\la\psi|O^\dagger|\phi\ra=\la\psi|O|\phi\ra,
\ee
finding that switching the left and right vector gives the complex conjugate of the original, and this will serve as the criteria for judging the hermitian property in the following.

So are $X$ and $D$ hermitian? We first deal with $X$,
\be\begin{split}
\la\phi|X|\psi\ra^*&=\1[\int\dif x\ \la\phi|x\ra\la x|X|\psi\ra\2]^*=\1[\int\dif x\ \phi^*(x)\ x \psi(x)\2]^*\\
&=\int\dif x\ \psi^*(x)\ x^* \phi(x)=\int\dif x\ \psi^*(x)\ x \phi(x)\\
&=\la\psi|X|\phi\ra,
\end{split}\ee
where we used the fact that the position variable is real. Thus the position operator is indeed Hermitian.

And next the $D$ operator,
\be\begin{split}
\la\phi|D|\psi\ra^*&=\1[\int\dif x\ \la\phi|x\ra\la x|D|\psi\ra\2]^*=\1[\int\dif x\ \phi^*(x)\  \frac\p{\p x}\psi(x)\2]^*\\
&=\1[\cancel{\int_{-\infty}^{\infty} \dif x\ \psi^*(x)\ \phi(x)}-\int\dif x\ \frac\p{\p x}\psi^*(x)\ \phi(x)\2]^*=-\int\dif x\ \phi^*(x)\  \frac\p{\p x}\psi(x)\\
&=-\la\psi|D|\phi\ra \implies D^\dagger=-D,
\end{split}\ee
where we have used the boundary condition that the wave functions vanish at infinity. Thus the $D$ operator is not Hermitian, but anti-Hermitian. But actually we still want to construct an Hermitian operator out of $D$, recall that the easiest way to construct an Hermitian operator out of an anti-Hermitian operator is multiplying it by $i$ or $-i$. So we define a new operator as
\be
P:=-i\hbar D,
\ee
and it's easy to check that $P$ is Hermitian,
\be
\la\phi|P|\psi\ra^*=[-i\hbar\la\phi|D|\psi\ra]^*=i\hbar[-\la\psi|D|\phi\ra]=\la\psi|P|\phi\ra,
\ee
and its basis-dependent counterpart is naturally given by $\hat p=-i\hbar\frac\p{\p x}$, which can act directly on the wave function,
\be
\hat p\psi(x)=-i\hbar\frac\p{\p x}\psi(x).
\ee\\

Next we explore the eigenvalues and eigenvectors of $X$ and $P$. Eigenvectors of an operator are, by definitions, vectors that acted by the operator gives the same vector multiplied by a number which is called the eigenvalue. For the $X$ operator, we are looking for a solution that satisfies,
\be
X|\psi\ra=x_0|\psi\ra,
\ee
where $x_0$ is a specific number. By definition, the LHS in $x$-basis is given by
\be
\la x|X|\psi\ra=x\la x|\psi\ra=x\psi(x),
\ee
and the RHS in the $x$-basis would be
\be
\la x|x_0|\psi\ra=x_0\la x|\psi\ra=x_0\psi(x).
\ee
So the equation we're going to solve is nothing but
\be
x\psi(x)=x_0\psi(x),
\ee
and when obtaining the expression for $\psi(x)$, we can easily recover the state vector by the superposition principle,$|\psi\ra=\int\dif x\ \psi(x)|x\ra$.\\
By rearranging the equation to solve, we get
\be
(x-x_0)\psi(x)=0,
\ee
which says that $\psi(x)$ could be non-zero only when $x=x_0$, or equivalently, when $x\ne x_0$, $\psi(x)$ must be zero. We summarize this as follows,
\be
\psi(x)= \begin{cases}
\text{(non-)zero}, & x=x_0 \\
0, & x\ne x_0,
\end{cases}
\ee
this reminds us of the Dirac-delta function $\delta(x-x_0)$, but we didn't strictly prove that $\psi(x)$ is indeed $\del(x-x_0)$. Since a straightforward proof is a little troublesome and will lead to some ambiguities (as I tried), we go around that and try to find another clever way. As all we want is a function that satisfies $x\psi(x)=x_0\psi(x)$, if we somehow happen to find such a function, we can say that it is the eigenfunction of the operator. The trick here is to use the relation between $X$ and $x$, as follows,
\be
x_0\del(x-x_0)=[x_0\del(x_0-x)]^*=\la x_0|X|x\ra^*=\la x|X|x_0\ra=x\del(x-x_0),
\ee
where we have used the Hermitian property of $X$, the realness of $x_0$ and $\del(x_0-x)$, and the evenness of $\del(x_0-x)$. Now we see that the Dirac-delta function $\del(x-x_0)$ indeed is a solution to the equation, so it is the eigenfunction, $\psi(x)=\delta(x-x_0)$, now we recover our eigenvector as
\be
|\psi\ra=\int\dif x\ \psi(x)|x\ra=\int\dif x\ \del(x-x_0) |x\ra =|x_0\ra,
\ee
and finally our eigen-equation will be
\be
X|x_0\ra=x_0|x_0\ra.
\ee
Since $x_0$ can be any value along the $x$-axis, this equation can be generalized to
\be
X|x\ra=x|x\ra,
\ee
which tells us the simple fact that any state representing a definite position is an eigenvector of the position operator $X$, and the eigenvalue is the position coordinte.\\

Then we play around with $P$. Similarly the eigen-equation is given by
\be
P|\psi\ra=p_0|\psi\ra,
\ee
and in $x$-basis,
\be
p_0\psi(x)=p_0\la x|\psi\ra=\la x|p_0|\psi\ra=\la x|P|\psi\ra=-i\hbar\frac\p{\p x}\la x|\psi\ra=-i\hbar\frac\p{\p x}\psi(x),
\ee
which is nothing but a simple first-order equation. The solution is given by
\be
\psi(x)=e^{\frac i \hbar p_0 x},
\ee
and the eigenvector is constructed as
\be
|\psi\ra=\int\dif x\ \psi(x)|x\ra=\int\dif x\ e^{\frac i \hbar p_0 x} |x\ra \equiv|p_0\ra,
\ee
which is just the Fourier-transformation of the state $|x\ra$. So the eigen-equation will be recovered as
\be
P|p_0\ra=p_0|p_0\ra \quad\text{or}\quad P|p\ra=p|p\ra,
\ee
where the physical meaning of $p$ and the operator $P$ will be clear later when we take the classical limit. As an aside, here we get a commonly used relation by re-express the solution,
\be
\la x|p\ra=\la x|\psi\ra=\psi(x)=e^{\frac i \hbar p x}, \quad\text{and}\quad \la p|x\ra=e^{-\frac i \hbar p x}.
\ee















\section{Time evolution of quantum states}
\subsection{Time evolution of isolated states and expectation values}

In classical mechanics, if two identical isolated systems start out in different states, they stay in different states. It underlies the fact that information is never lost. The same is true in quantum mechanics, and even much stronger. The direct manifestation is that if two states are initially orthogonal, they remain orthogonal throughout the evolution, or \textit{conservation of orthogonality}.

We denote a state at time $t$ as $|\Psi(t)\ra$, which originates from the state at time $t=0$, $|\Psi(0)\ra$, by acting an \textit{time-evolution operator} $\hat U(t)$ on it,
\be
|\Psi(t)\ra=\hat U(t) |\Psi(0)\ra.
\ee
This equation reflects the fact that quantum evolution of states allows us to compute the probabilities of the outcomes of later experiments, in contract to the case in classical mechanics, classical determinism allows us to predict the results of experiments. 

Mathematically, the conservation of orthogonality tells us
\be
\la \Psi(0)|\Phi(0)\ra=0 \implies \la \Psi(t)|\Phi(t)\ra=\la \Psi(0)|\hat U^\dagger(t) \hat U(t)|\Phi(0)\ra=0,
\ee
if we choose $|\Psi(0)\ra$ and $|\Phi(0)\ra$ to be any two orthonormal basis of vectors $|i\ra$ and $|j\ra$, we could obtain a property of the evolution operator,
\be
\la i|j\ra =\delta_{ij} = \la i|\hat U^\dagger(t) \hat U(t)|j\ra \implies \hat U^\dagger(t) \hat U(t)=I,
\ee
with which the evolution operator satisfies is called \textit{unitary}.\\

\textit{Principle: The evolution of state-vectors with time is unitary.}\\

A direct consequence of the unitarity leads to a much stronger version of conservation of orthogonality --- \textit{conservation of overlaps},
\be
\la A(t)|B(t)\ra = \la A(0) |\hat U^\dagger(t)\hat U(t)|B(0)\ra = \la A(0)|B(0)\ra,
\ee
which says the inner product of $|A\ra$ and $|B\ra$ does not change with time, or the inner product is conserved. Thus superficially conservation of orthogonality is a special case of conservation of overlap, we have seen that conservation of orthogonality can lead to conservation of overlap, thus they are equivalent.\\

Time evolution is led by incremental changes, so it would be inspiring to study the evolution operator of an infinitesimal time interval $\epsilon$. Another property besides unitarity that an evolution operator should have is \textit{continuity}, which means that the state-vector changes smoothly, or to be more explicitly, when $\epsilon$ is very small, $\hat U(\epsilon)$ is close to the unit operator, only differing from it by something of order $\epsilon$, and when $\ep$ is taken to be zero, the state does not change. 
\be
U(\epsilon)=I-i\epsilon H,
\ee
where the $-i$ and the operator $H$ is arbitrary and conventional. By plugging its Hermitian conjugate form
\be
U^\dagger(\epsilon)=I+i\epsilon H^\dagger
\ee
into the unitarity equation and expanding it to the first power, we have
\be
I=U^\dagger(\epsilon)U(\epsilon)=(I+i\epsilon H^\dagger)(I-i\epsilon H)=I+i\epsilon (H^\dagger-H),
\ee
or
\be
H^\dagger=H.
\ee
This Hermitian property of  $H$ has the great significance that $H$ is an observable and has a complete set of orthonormal eigenvalues and eigenvectors.\\

{\color{blue}Here I want to explain the above ansatz in more detail, which is due to my own understanding and I think is more natural.\\
The most general form of a state obtained by acting the infinitesimal evolution operator on the initial state, considering continuity, takes the form as
\be
U(\ep)|\Psi\ra=|\Psi\ra+\ep|\Theta\ra,
\ee
where $|\Theta\ra$ is some unknown state though, it can be obtained by acting some operator to the original state $|\Theta\ra\equiv A|\Psi\ra$, so we have
\be
U(\ep)|\Psi\ra=|\Psi\ra+\ep A|\Psi\ra =(I+\ep A)|\Psi\ra,
\ee
then we have a form of $U(\ep)$ as
\be
U(\epsilon)=I+\ep A,
\ee
where the property and meaning of $A$ is to be explored. Using the unitarity, we have
\be
I=U^\dagger(\epsilon)U(\epsilon)=(I+\ep A^\dagger)(I+\ep A)=I+\epsilon (A^\dagger+A),
\ee
or
\be
A^\dagger=-A,
\ee
which says $A$ is anti-Hermitian.\\
But it is easy to construct an Hermitian operator, which is more desired by us, out of the anti-Hermitian operator just by multiplying the imaginary unit on it, ie, 
\be\ma H\equiv iA =(-i)(-A)= (iA)^\dagger = \ma H^\dagger,\ee
thus we have the form of the infinitesimal evolution operator as
\be
U(\epsilon)=I+\ep A=I+\epsilon (-i)(iA)=1-i\ep\ma H,
\ee
and due to the Hermitian property of $\ma H$, it should be some kind of observable.\\}

Now we're going to explore more on the evolution of states at an infinitesimal time interval $\epsilon$, we write
\be
|\Psi(\ep)\ra=\hat U(\ep) |\Psi(0)\ra = |\Psi(0)\ra-i\ep H|\Psi(0)\ra,
\ee
and rearranging this we have
\be
\frac{|\Psi(\ep)\ra-|\Psi(0)\ra}{\ep}=-iH|\Psi(0)\ra,
\ee
where the LHS is nothing but the time derivative of the state $|\Psi\ra$ at time $t=0$, but since the evolution is assumed to be linear, this should hold at any time $t$, so we may write
\be
\frac{\dif |\Psi\ra}{\dif t}=-i H|\Psi\ra,
\ee
where we wrote partial or total derivative make no difference. This equation tells us how an isolated (not contact with apparatus) state evolves with time, and it is called the \textit{generalized Schrodinger equation}. This is a simple first-order differential equation, and its solution is given by
\be
|\Psi(t)\ra=e^{-iHt}|\Psi(0)\ra,
\ee
thus we may extract the explicit form of the evolution operator as $U(t)=e^{-iHt}$, and we check it by taking the time to be infinitesimal time interval $\ep$,
\be
U(t)=e^{-iH\ep}\approx 1-iH\ep,
\ee
which is consistent with our previous reasoning.

As we proceed, we would discover that $H$ is nothing but the Hamiltonian operator, thus it has the dimension of energy. By checking the dimensionality of the time-dependent Schrodinger equation, we may fix it by a factor of $\hbar$, the Planck constant,
\be
i\hbar\frac{\p |\Psi\ra}{\p t}= H|\Psi\ra.
\ee
This is the standard form of the \textit{time-dependent Schrodinger equation}.\\

Now that we have derived that $H$ is a Hermitian operator, we can expand our states in the basis of $H$'s eigenvectors, to fully solve the Schrodinger equation,
\be
|\Psi\ra=\sum_j\al_j|E_j\ra,
\ee
where $|E_i\ra$ satisfies
\be
H|E_i\ra=E_i|E_i\ra,
\ee
and $E_i$s are the eigenvalues of $H$, or energy.\\
Using the time-dependent Schrodinger equation and due the fact that basis vector do not change with time, we have
\be
\sum_j\dot\al_j(t)|E_j\ra=-\frac i \hbar H\sum_j\al_j(t)|E_j\ra=-\frac i \hbar \sum_jE_j\al_j(t)|E_j\ra,
\ee
or regrouping
\be
\sum_j\bigg\{\dot\al_j(t)+\frac i \hbar E_j\al_j\bigg\}|E_j\ra=0,
\ee
thus every coefficient must be zero, and we have again the simplest differential equation,
\be\frac{\dif \al_j(t)}{\dif t}=-\frac i \hbar E_j \al_j(t)\ee,
and the solution is given by
\be\al_j(t)=\al_j(0)e^{-\frac i\hbar E_j t}\ee.
This equation contains the underlying assumption that Hamiltonian does not depend explicitly on time. And it's the first time for us to see the deep connection between energy and time.
We may extract out coefficients at time zero as \be\al_j(0)=\la E_j|\Psi(0)\ra\ee, thus we can write the full solution fo the time-dependent Schrodinger equation as
\be
|\Psi(t)\ra=\sum_j\al_j(0)e^{-\frac i\hbar E_jt}|E_j\ra=\sum_j|E_j\ra\la E_j|\Psi(0)\ra e^{-\frac i\hbar E_jt},
\ee
which emphasizes that we're summing over the basis vectors, and implies that as long as we have the initial state $|\Psi(0)\ra$ and the Hamiltonian $H$ which governs the physical law of evolution, we can have state $|\Psi(t)\ra$ at any time as we want.\\

The average, or expectation value, of an observable is the closest thing in quantum mechanics to a classical value. From a mathematical point of view, an average is defined by the equation
\be
\la\lam \ra = \sum_i \lam_i P(\lam_i).
\ee
If we expand a quantum state $|\Psi\ra$ in the orthonormal basis of eigenvectors of $L$,
\be
|\Psi\ra=\sum_i\al_i|\lam_i\ra,
\ee
we try to compute the following quantity,
\be
\la L \ra\equiv\la \Psi|L|\Psi\ra=\sum_i\sum_{j}\la \lam_j| \al^*_j \lam_i \al_i|\lam_i\ra=\sum_i  \lam_i (\al^*_i\al_i)= \sum_i \lam_i P(\lam_i)=\la \lam \ra,
\ee
that's why we use the notation $\la L\ra$ to denote the average. Let's see how this changes with time,
\bs
\frac\dif{\dif t}\la L \ra=\frac\dif{\dif t}\la \Psi(t)|L|\Psi(t)\ra&=\la \dot\Psi(t)|L|\Psi(t)\ra+\la \Psi(t)|L|\dot\Psi(t)\ra\\
&=\frac i \hbar \la \Psi(t)|(HL-LH)|\Psi(t)\ra=\frac i \hbar\la [H,L]\ra,
\end{split}\ee
where in the second row we used the Schrodinger equation, and the notation for a commutator. Note that the $i$ before the commutator is important, since the commutator itself is always imaginary.\\
Equivalently we rewrite it in a shorthand form:
\be\label{dl}
\frac{\dif L}{\dif t}=-\frac i \hbar [L,H],
\ee
where letters denote their average value. This is a very interesting and important equation. It relates the time derivative of the expectation value of an observable $L$ to the expectation value of another observable $[L,H]$. If we assume that the probabilities are nice, narrow, bell-shaped curves, this equation tells us how the peaks of the curves move with time. Equations like this are the closest thing in quantum mechanics to the equations of classical physics.\\
Recall that in classical mechanics, the time derivative of a physical quantity is given by its Poisson Bracket with the Hamiltonian,
\be
\frac{\dif L}{\dif t}=\{L,H\}.
\ee
Due to their similarity, we may identify a correspondence between the commutators and the Poisson Brackets,
\be
[L,H] \iff i\hbar\{L,H\}.
\ee
{\color{blue}Actually they have the same mathematical structures (like anti-symmetric, Bianchi identity) called the Lie Algebra, which would be explored later if we have time.}\\
In classical physics, commutators between ordinary observables are zero. But if we want to apply \eqref{dl} to classical physics, we must assume the commutators are not zero but a very small value of the order of $\hbar$. Vice versa, the classical limit is the limit at which $\hbar$ is negligibly small, thus the $i\hbar\{L,H\}$ goes back to the vanishing commutator.\\

Similar to classical mechanics, the equation tells us the expectation value of an observable $\la Q\ra$  does not change with time if it commutes with the Hamiltonian,
\be
\frac{\dif\bar Q}{\dif t}=-\frac i \hbar\overline{[Q,H]} =0.
\ee
Moreover if $Q$ commutes with the Hamiltonian, the expectation values of all powers of $Q$, and even all functions of $Q$ are conserved.\\
Just like in classical mechanics, the most obvious conserved quantity is the Hamiltonian itself, since it commutes with itself,
\be
\frac{\dif\bar H}{\dif t}=-\frac i \hbar\overline{[H,H]}=0,
\ee
which says energy is conserved in quantum mechanics.\\

Now it's more concrete to set the spin as an example. The energy of a spin in a magnetic field is given by
\be
H\sim \vec\sig\cdot\vec B=\sig_x B_x+\sig_y B_y+\sig_z B_z.
\ee
If the magnetic field lies along the z-axis, the Hamiltonian is proportional to $\sig_z$, in the following expression we'll absorb all the irrelevant numerical constants into a single constant $\omega$, so
\be 
H=\frac{\hbar\omega}{2} \sig_z,
\ee
where we kept $\frac\hbar 2$ for later convenience.\\
Now we want to find out how the expectation value of the spin varies with time, we can get
\be\begin{split}
\dot{\la\sig_x\ra}&=-\frac i \hbar \la[\sig_x,H]\ra=-\frac{i\omega} 2\la[\sig_x,\sig_z]\ra=-\om\la\sig_y\ra\\
\dot{\la\sig_y\ra}&=-\frac i \hbar \la[\sig_y,H]\ra=-\frac{i\omega} 2\la[\sig_y,\sig_z]\ra=\om\la\sig_x\ra\\
\dot{\la\sig_z\ra}&=-\frac i \hbar \la[\sig_z,H]\ra=-\frac{i\omega} 2\la[\sig_z,\sig_z]\ra=0
\end{split}\ee
This result implies that the 3-vector-operator $\vec\sig$ precesses like a gyroscope around the direction of the magnetic field. The precession is uniform, with angular velocity $\om$. To be specific, the expectation value for a $\sig_z$ measurement does not change with time, but the other two expectation values do change.\\

All the above formalism in this subsection describes how state evolves with time during two measurements. But something different happens when an observation is made, during an experiment the state of a system jumps unpredictably to an eigenstate of the observable that was measured. This phenomenon is called \textit{the collapse of the wave function}. This implies that to examine the measurement process itself as a quantum mechanical evolution, we must consider the entire experimental setup, including the apparatus, as part of a single quantum system.
















\end{document}