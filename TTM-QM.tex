\documentclass{article}

\usepackage{amsmath,mathtools}
\usepackage{bm,extarrows,ulem}
\usepackage{mathrsfs}
\usepackage{geometry,graphicx,color}
\geometry{centering,scale=0.8}

\newcommand{\be}{\begin{equation}}
\newcommand{\ee}{\end{equation}}
\newcommand{\bea}{\begin{eqnarray}}
\newcommand{\eea}{\end{eqnarray}}
\newcommand{\ba}{\begin{array}}
\newcommand{\ea}{\end{array}}
\newcommand{\bs}{\be\begin{split}}

\newcommand{\dif}{\,\mathrm{d}}
\newcommand{\p}{\partial}
\newcommand{\1}{\left}
\newcommand{\2}{\right}
\newcommand{\ma}{\mathcal}
\newcommand{\br}{\langle}
\newcommand{\ke}{\rangle}

\newcommand{\m}{\mu}
\newcommand{\n}{\nu}
\newcommand{\al}{\alpha}
\newcommand{\bet}{\beta}
\newcommand{\lam}{\lambda}
\newcommand{\sig}{\sigma}
\newcommand{\ep}{\epsilon}


\title{Notes on The Theoretical Minimum\\
--- Quantum Mechanics}
\author{Gui-Rong Liang}

\begin{document}
\maketitle
\tableofcontents

\newpage

\section{Mathematical Foundations and Spins}
\subsection{Quantum states, operators, and Hermitian properties}
A quantum state is represented by a ket vector that can be decomposed using the orthogonal basis vectors,
\be
|A\ke=\sum_j \al_j|j\ke, \quad\text{with} \quad \al_j=\br j|A\ke,
\ee
where the coefficient $\al_j$ is a complex number. Thus
\be
|A\ke=\sum_j |j\ke \br j|A\ke \implies  \sum_j |j\ke \br j|=1.
\ee
The probability of measuring a state corresponding to a certain basis is given by
\be
P_j=\al_j^*\al_j=\br A |j\ke \br j|A\ke,
\ee
then the normalization of probability gives to 
\be
1=\sum_j P_j=\sum_j\br A |j\ke \br j|A\ke=\br A|A\ke.
\ee
So the general principle is the state of a system is represented by a unit vector.\\

Now we introduce the matrix representation of an operator, which turn a vector to another vector,
\be
M|A\ke=|B\ke.
\ee
If we decompose A and B into components on basis vectors, we can represent the above equation as,
\be
\sum_j M |j\ke \al_j=\sum_j \bet_j|j\ke,
\ee
multiplying the above by a basis bra, we have
\be
\sum_j \br k| M |j\ke \al_j=\sum_j \bet_j \br k|j\ke = \bet_k,
\ee
which is denoted by 
\be
\sum_j M_{kj} \al_j = \bet_k,
\ee
where $M_{ij}\equiv \br k|M|j\ke$ is matrix element of the operator.\\
We write the above as explicit matrix form (taking 2 dimension as an example) as
\be
\1(\ba{cc}M_{11}&M_{12}\\M_{21}&M_{22}\ea\2)\1(\ba{c}\al_1\\\al_2\ea\2)=\1(\ba{c}\bet_1\\\bet_2\ea\2).
\ee
Now we take its complex conjugate and make it a equation of row vectors,
\be
\1(\ba{cc}\al_1^*&\al_2^*\ea\2)\1(\ba{cc}M_{11}^*&M_{21}^*\\M_{12}^*&M_{22}^*\ea\2)=\1(\ba{cc}\bet^*_1&\bet^*_2\ea\2),
\ee
which can be represented by the bras notation,
\be
\br A|M^\dagger =\br B|,
\ee
where $M^\dagger$ is called the \textit{Hermitian conjugate} of $M$, with the elements of $M^\dagger$ is given by
\be
(M^\dagger)_{jk}=M^*_{kj}.
\ee
An Hermitian operator is an operator which equals to its own Hermitian conjugate,
\be
M=M^\dagger.
\ee
Physical observable quantities are represented by Hermitian operators, due to the fact that the eigenvalues of Hermitian operators are real. And further, eigenvectors of an Hermitian operator corresponding to different eigenvalues are orthogonal. Next we will prove this.\\
If
\be
L|\lam\ke=\lam|\lam\ke,
\ee
where $L$ is Hermitian, we have
\be
\lam^*\br\lam|\lam\ke=\br\lam|L^\dagger|\lam\ke=\br\lam|L|\lam\ke=\lam\br\lam|\lam\ke \implies \lam^*=\lam,
\ee
Thus $\lam$ is real. Then if
\be\1\{\begin{split}
L|\lam_1\ke&=\lam_1|\lam_1\ke \\
L|\lam_2\ke&=\lam_2|\lam_2\ke,
\end{split}\2.\ee
we have
\be
\lam_1\br\lam_1|\lam_2\ke=\br\lam_1|L|\lam_2\ke=\lam_2\br\lam_1|\lam_2\ke \implies (\lam_1-\lam_2)\br\lam_1|\lam_2\ke=0 \implies \br\lam_1|\lam_2\ke=0.
\ee
Even if eigenvectors of the same eigenvalue are not orthogonal, we can make it orthogonal by Gram-Schmidt procedure. Finally we can make all eigenvectors of an Hermitian operator orthogonal, thus they can be used as a basis for the vector space.\\

\subsection{Time evolution of states and expectation values}

In classical mechanics, if two identical isolated systems start out in different states, they stay in different states. It underlies the fact that information is never lost. The same is true in quantum mechanics, and even much stronger. The direct manifestation is that if two states are initially orthogonal, they remain orthogonal throughout the evolution, or \textit{conservation of orthogonality}.

We denote a state at time $t$ as $|\Psi(t)\ke$, which originates from the state at time $t=0$, $|\Psi(0)\ke$, by acting an \textit{time-evolution operator} $\hat U(t)$ on it,
\be
|\Psi(t)\ke=\hat U(t) |\Psi(0)\ke.
\ee
This equation reflects the fact that quantum evolution of states allows us to compute the probabilities of the outcomes of later experiments, in contract to the case in classical mechanics, classical determinism allows us to predict the results of experiments. 

Mathematically, the conservation of orthogonality tells us
\be
\br \Psi(0)|\Phi(0)\ke=0 \implies \br \Psi(t)|\Phi(t)\ke=\br \Psi(0)|\hat U^\dagger(t) \hat U(t)|\Phi(0)\ke=0,
\ee
if we choose $|\Psi(0)\ke$ and $|\Phi(0)\ke$ to be any two orthonormal basis of vectors $|i\ke$ and $|j\ke$, we could obtain a property of the evolution operator,
\be
\br i|j\ke =\delta_{ij} = \br i|\hat U^\dagger(t) \hat U(t)|j\ke \implies \hat U^\dagger(t) \hat U(t)=I,
\ee
with which the evolution operator satisfies is called \textit{unitary}.\\

\textit{Principle: The evolution of state-vectors with time is unitary.}\\

A direct consequence of the unitarity leads to a much stronger version of conservation of orthogonality --- \textit{conservation of overlaps},
\be
\br A(t)|B(t)\ke = \br A(0) |\hat U^\dagger(t)\hat U(t)|B(0)\ke = \br A(0)|B(0)\ke,
\ee
which says the inner product of $|A\ke$ and $|B\ke$ does not change with time, or the inner product is conserved.\\

Time evolution is led by incremental changes, so it would be inspiring to study the evolution operator of an infinitesimal time interval $\epsilon$. Another property besides unitarity that an evolution operator should have is \textit{continuity}, which means that the state-vector changes smoothly, or to be more explicitly, when $\epsilon$ is very small, $\hat U(\epsilon)$ is close to the unit operator, only differing from it by something of order $\epsilon$, and when $\ep$ is taken to be zero, the state does not change. 
\be
U(\epsilon)=I-i\epsilon H,
\ee
where the $-i$ and the operator $H$ is arbitrary and conventional. By plugging its Hermitian conjugate form
\be
U^\dagger(\epsilon)=I+i\epsilon H^\dagger
\ee
into the unitarity equation and expanding it to the first power, we have
\be
I=U^\dagger(\epsilon)U(\epsilon)=(I+i\epsilon H^\dagger)(I-i\epsilon H)=I+i\epsilon (H^\dagger-H),
\ee
or
\be
H^\dagger=H.
\ee
This Hermitian property of  $H$ has the great significance that $H$ is an observable and has a complete set of orthonormal eigenvalues and eigenvectors.\\

{\color{blue}Here I want to explain the above ansatz in more detail, which is due to my own understanding and I think is more natural.\\
The most general form of a state obtained by acting the infinitesimal evolution operator on the initial state, considering continuity, takes the form as
\be
U(\ep)|\Psi\ke=|\Psi\ke+\ep|\Theta\ke,
\ee
where $|\Theta\ke$ is some unknown state though, it can be obtained by acting some operator to the original state $|\Theta\ke\equiv A|\Psi\ke$, so we have
\be
U(\ep)|\Psi\ke=|\Psi\ke+\ep A|\Psi\ke =(I+\ep A)|\Psi\ke,
\ee
then we have a form of $U(\ep)$ as
\be
U(\epsilon)=I+\ep A,
\ee
where the property and meaning of $A$ is to be explored. Using the unitarity, we have
\be
I=U^\dagger(\epsilon)U(\epsilon)=(I+\ep A^\dagger)(I+\ep A)=I+\epsilon (A^\dagger+A),
\ee
or
\be
A^\dagger=-A,
\ee
which says $A$ is anti-Hermitian.\\
We argue that any operator can be make up of two Hermitian operator $F$ and $H$, which serve as real and imaginary part individually, as $A=F-iH$, (just take $F$ and $H$ as they have the same eigenvectors as $A$, and eigenvalues as real and imaginary part of the eigenvalues of $A$), then it's easy to figure out that an anti-Hermitian operator has no real part, i.e, $A=-iH$, thus we have the form of the infinitesimal evolution operator as
\be
U(\epsilon)=I-i\epsilon H,
\ee
and due to the Hermitian property of $H$, it should be some kind of observable.\\}

Now we're going to explore more on the evolution of states at an infinitesimal time interval $\epsilon$, we write
\be
|\Psi(\ep)\ke=\hat U(\ep) |\Psi(0)\ke = |\Psi(0)\ke-i\ep|\Psi(0)\ke,
\ee
and rearranging this we have
\be
\frac{|\Psi(\ep)\ke-|\Psi(0)\ke}{\ep}=-iH|\Psi(0)\ke,
\ee
where the LHS is nothing but the time derivative of the state $|\Psi\ke$ at time $t=0$, but since the evolution is assumed to be linear, this should hold at any time $t$, so we may write
\be
\frac{\p |\Psi\ke}{\p t}=-i H|\Psi\ke,
\ee
where we wrote partial or total derivative make no difference. This equation tells us how an isolated (not contact with apparatus) state evolves with time, and it is called the \textit{generalized Schrodinger equation}.

As we proceed, we would discover that $H$ is nothing but the Hamiltonian operator, thus it has the dimension of energy. By checking the dimensionality of the time-dependent Schrodinger equation, we may fix it by a factor of $\hbar$, the Planck constant,
\be
i\hbar\frac{\p |\Psi\ke}{\p t}= H|\Psi\ke.
\ee
This is the standard form of the \textit{time-dependent Schrodinger equation}.\\

The average, or expectation value, of an observable is the closest thing in quantum mechanics to a classical value. From a mathematical point of view, an average is defined by the equation
\be
\br\lam \ke = \sum_i \lam_i P(\lam_i).
\ee
If we expand a quantum state $|\Psi\ke$ in the orthonormal basis of eigenvectors of $L$,
\be
|\Psi\ke=\sum_i\al_i|\lam_i\ke,
\ee
we try to compute the following quantity,
\be
\br L \ke\equiv\br \Psi|L|\Psi\ke=\sum_i\sum_{j}\br \lam_j| \al^*_j \lam_i \al_i|\lam_i\ke=\sum_i  \lam_i (\al^*_i\al_i)= \sum_i \lam_i P(\lam_i)=\br \lam \ke,
\ee
that's why we use the notation $\br L\ke$ to denote the average. Let's see how this changes with time,
\bs
\frac\dif{\dif t}\br L \ke=\frac\dif{\dif t}\br \Psi(t)|L|\Psi(t)\ke&=\br \dot\Psi(t)|L|\Psi(t)\ke+\br \Psi(t)|L|\dot\Psi(t)\ke\\
&=\frac i \hbar \br \Psi(t)|(HL-LH)|\Psi(t)\ke=\frac i \hbar\br [H,L]\ke,
\end{split}\ee
where in the second row we used the Schrodinger equation, and the notation for a commutator. Note that the $i$ before the commutator is important, since the commutator itself is not Hermitian.\\
Equivalently we rewrite it in a shorthand form:
\be
\frac{\dif L}{\dif t}=-\frac i \hbar [L,H],
\ee
where letters denote their average value. This is a very interesting and important equation. It relates the time derivative of the expectation value of an observable $L$ to the expectation value of another observable $[L,H]$. If we assume that the probabilities are nice, narrow, bell-shaped curves, this equation tells us how the peaks of the curves move with time. Equations like this are the closest thing in quantum mechanics to the equations of classical physics.

\subsection{Spin states, spin operators, and Pauli Matrices}
\textit{All possible spin states can be represented in a two dimensional vector space.}

\be
|A\ke=\al_u|u\ke+\al_k|d\ke,
\ee
where the $u$ and $d$ represents for pointing up and down when we measure spin along z-axis, and the orthogonality means if we get up we never get down, vice versa.\\
Now we choose A to be pointing left and right, considering that they're orthonormal, and it is equally likely to be up and down when measured along z, we write them as
\be\1\{\begin{split}
|r\ke&=\frac1 {\sqrt{2}}|u\ke+\frac1 {\sqrt{2}}|d\ke\\
|l\ke&=\frac1 {\sqrt{2}}|u\ke- \frac1 {\sqrt{2}}|d\ke.
\end{split}\2.\ee
And when we represent forward and backward, we must further take into account the equal likelihood of left and right when measured along x-axis, so we take the following form,
\be\1\{\begin{split}
|f\ke&=\frac1 {\sqrt{2}}|u\ke+\frac i {\sqrt{2}}|d\ke\\
|b\ke&=\frac1 {\sqrt{2}}|u\ke- \frac i {\sqrt{2}}|d\ke.
\end{split}\2.\ee
Notice that all the above choices have the same freedom by a phase factor $e^{i\theta}$.

We will use $\sig$ to denote spin operator. Measuring along z-axis gives us
\be\1\{\begin{split}
\sig_z |u\ke&=|u\ke\\
\sig_z |d\ke&=-|d\ke,
\end{split}\2.\ee
if we denote $|u\ke$ and $|d\ke$ as unit vectors as $\1(\ba{c}1\\0\ea\2)$ and $\1(\ba{c}0\\1\ea\2)$, we may figure out that
\be
\sig_z=\1(\ba{cc}1&0\\0&-1\ea\2).
\ee
Measuring along x-axis gives us
\be\1\{\begin{split}
\sig_x |r\ke&=|r\ke\\
\sig_x |l\ke&=-|l\ke,
\end{split}\2.\ee
if we denote $|r\ke$ and $|l\ke$ as unit vectors as $\1(\ba{c}\frac1{\sqrt{2}}\\\frac1{\sqrt{2}}\ea\2)$ and $\1(\ba{c}\frac1{\sqrt{2}}\\-\frac1{\sqrt{2}}\ea\2)$, we may figure out that
\be
\sig_x=\1(\ba{cc}0&1\\1&0\ea\2).
\ee
Measuring along y-axis gives us
\be\1\{\begin{split}
\sig_y |f\ke&=|f\ke\\
\sig_y |b\ke&=-|b\ke,
\end{split}\2.\ee
if we denote $|f\ke$ and $|b\ke$ as unit vectors as $\1(\ba{c}\frac1{\sqrt{2}}\\\frac i{\sqrt{2}}\ea\2)$ and $\1(\ba{c}\frac1{\sqrt{2}}\\-\frac i{\sqrt{2}}\ea\2)$, we may figure out that
\be
\sig_x=\1(\ba{cc}0&-i\\i&0\ea\2).
\ee
The matrix representations of $\sig_x$, $\sig_y$ and $\sig_z$ are called the \textit{Pauli Matrices}.\\

Now we may treat $\sig_x$, $\sig_y$ and $\sig_z$ as three components of $\sig$, thus $\sig$ can be viewed as a general vector, and it can be projected onto a unit vector $\hat n$ point any direction in space, so as to measure spin along that direction.
\be
\sig_n=\vec\sig\cdot\hat n=\sig_xn_x+\sig_yn_y+\sig_zn_z=\1(\ba{cc}n_z&n_x-in_y\\n_x+in_y&-n_z\ea\2)
\ee

If $\hat n$ lies x-z plane, we may write
\be\1\{\begin{split}
n_z&=\cos\theta\\
n_x&=\sin\theta\\
n_y&=0,
\end{split}\2.\ee
thus 
\be
\sig_n=\1(\ba{cc}\cos\theta&\sin\theta\\\sin\theta&-\cos\theta\ea\2),
\ee
its eigenvalues and corresponding eigenvectors are given by
\be\1\{\begin{split}
\lam_1=1, &\quad|\lam_1\ke=\1(\ba{c}\cos{\frac\theta 2}\\\sin{\frac\theta 2}\ea\2)\\
\lam_2=-1, &\quad|\lam_1\ke=\1(\ba{c}-\sin{\frac\theta 2}\\\cos{\frac\theta 2}\ea\2).
\end{split}\2.\ee
Now suppose we prepare a $|u\ke$ state, and we measure the probabilities of $+1$ and $-1$ along the $\hat n$ direction, we get
\be\1\{\begin{split}
P(+1)=&\br u|\lam_1\ke\br\lam_1|u\ke=\cos^2{\frac{\theta}{2}}\\
P(-1)=&\br u|\lam_2\ke\br\lam_2|u\ke=\sin^2{\frac{\theta}{2}}
\end{split}\2.\ee
Now we're ready to compute the average value of spin along the $n$ direction,
\be
\br \sig_n\ke=\sum_j \lam_j P(\lam_j)=\cos^2{\frac{\theta}{2}}-\sin^2{\frac{\theta}{2}}=\cos{\theta},
\ee
which agrees perfectly with our expectation.\\

To proceed, we come to an important theorem,\\

\textit{\textbf{The Spin-Polarization Principle:} Any state of a single spin is an eigenvector of some component of the spin.}\\

In other words, given any state
\be
|A\ke=\al_u|u\ke+\al_d|d\ke,
\ee
there exists some direction $\hat n$, such that
\be
\vec\sig\cdot\vec n|A\ke= |A\ke.
\ee
In physics language, we say that the states of a spin are characterized by a \textit{polarization vector}, and along that polarization vector the component of spin is predictably $+1$, assuming of course that you know the state-vector.\\ It follows that the expectation value of the spin along the direction $\hat n$ can be expressed as
\be
\br\sig_n\ke=\br \vec\sig\cdot\vec n\ke=1.
\ee


















\end{document}