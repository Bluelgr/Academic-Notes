\documentclass{article}

\usepackage{amsmath,mathtools,amssymb}
\usepackage{bm,extarrows,ulem,cancel}
\usepackage{mathrsfs}
\usepackage{geometry,graphicx,color}
\geometry{centering,scale=0.8}
\usepackage[all,pdf]{xy}
\usepackage{pstricks,pstricks-add}


\newcommand{\sect}{\section}
\newcommand{\subsec}{\subsection}

\newcommand{\be}{\begin{equation}}
\newcommand{\ee}{\end{equation}}
\newcommand{\bea}{\begin{eqnarray}}
\newcommand{\eea}{\end{eqnarray}}
\newcommand{\ba}{\begin{array}}
\newcommand{\ea}{\end{array}}
\newcommand{\bs}{\be\begin{split}}

\newcommand{\dif}{\,\mathrm{d}}
\newcommand{\p}{\partial}
\renewcommand{\1}{\left}
\renewcommand{\2}{\right}
\newcommand{\ma}{\mathcal}
\newcommand{\la}{\langle}
\newcommand{\ra}{\rangle}

\newcommand{\m}{\mu}
\newcommand{\n}{\nu}
\newcommand{\al}{\alpha}
\newcommand{\bet}{\beta}
\newcommand{\lam}{\lambda}
\newcommand{\sig}{\sigma}
\newcommand{\ep}{\epsilon}
\newcommand{\om}{\omega}
\newcommand{\del}{\delta}
\newcommand{\Del}{\Delta}
\renewcommand{\th}{\theta}


\title{Notes on The Theoretical Minimum\\
--- Quantum Mechanics, Advanced Quantum Mechanics, and Quantum Field Theory}
\author{Gui-Rong Liang}

\begin{document}
\maketitle
\tableofcontents

\newpage

\section{Mathematical Foundations and Spin systems}
\subsection{Basic concepts and fundamental principles}
1. Quantum states, operators, Hermitian properties\\

A quantum state is represented by a ket vector that can be decomposed using the orthogonal basis vectors, in this subsection we mainly focus on discrete basis, thus 
\be
|A\ra=\sum_j \al_j|j\ra, \quad\text{with} \quad \al_j=\la j|A\ra,
\ee
where the coefficient $\al_j$ is a complex number, and is also called a wave function. Thus
\be
|A\ra=\sum_j |j\ra \la j|A\ra \implies  \sum_j |j\ra \la j|=1.
\ee
The probability of measuring a state corresponding to a certain basis is given by
\be
P_j=\al_j^*\al_j=\la A |j\ra \la j|A\ra,
\ee
then the normalization of probability gives to 
\be
1=\sum_j P_j=\sum_j\la A |j\ra \la j|A\ra=\la A|A\ra.
\ee
So the general principle is the state of a system is represented by a unit vector.\\

Now we introduce the matrix representation of an operator, which turn a vector to another vector,
\be
M|A\ra=|B\ra.
\ee
If we decompose A and B into components on basis vectors, we can represent the above equation as,
\be
\sum_j M |j\ra \al_j=\sum_j \bet_j|j\ra,
\ee
multiplying the above by a basis bra, we have
\be
\sum_j \la k| M |j\ra \al_j=\sum_j \bet_j \la k|j\ra = \bet_k,
\ee
which is denoted by 
\be
\sum_j M_{kj} \al_j = \bet_k,
\ee
where $M_{ij}\equiv \la k|M|j\ra$ is matrix element of the operator.\\
We write the above as explicit matrix form (taking 2 dimension as an example) as
\be
\1(\ba{cc}M_{11}&M_{12}\\M_{21}&M_{22}\ea\2)\1(\ba{c}\al_1\\\al_2\ea\2)=\1(\ba{c}\bet_1\\\bet_2\ea\2).
\ee
Now we take its complex conjugate and make it a equation of row vectors,
\be
\1(\ba{cc}\al_1^*&\al_2^*\ea\2)\1(\ba{cc}M_{11}^*&M_{21}^*\\M_{12}^*&M_{22}^*\ea\2)=\1(\ba{cc}\bet^*_1&\bet^*_2\ea\2),
\ee
which can be represented by the bras notation,
\be
\la A|M^\dagger =\la B|,
\ee
where $M^\dagger$ is called the \textit{Hermitian conjugate} of $M$, with the elements of $M^\dagger$ is given by
\be
(M^\dagger)_{jk}=M^*_{kj}.
\ee
An Hermitian operator is an operator which equals to its own Hermitian conjugate,
\be
M=M^\dagger.
\ee
Physical observable quantities are represented by Hermitian operators, due to the fact that the eigenvalues of Hermitian operators are real. And further, eigenvectors of an Hermitian operator corresponding to different eigenvalues are orthogonal. Next we will prove this.\\
If
\be
L|\lam\ra=\lam|\lam\ra,
\ee
where $L$ is Hermitian, we have
\be
\lam^*\la\lam|\lam\ra=\la\lam|L^\dagger|\lam\ra=\la\lam|L|\lam\ra=\lam\la\lam|\lam\ra \implies \lam^*=\lam,
\ee
Thus $\lam$ is real. Then if
\be\1\{\begin{split}
L|\lam_1\ra&=\lam_1|\lam_1\ra \\
L|\lam_2\ra&=\lam_2|\lam_2\ra,
\end{split}\2.\ee
we have
\be
\lam_1\la\lam_1|\lam_2\ra=\la\lam_1|L|\lam_2\ra=\lam_2\la\lam_1|\lam_2\ra \implies (\lam_1-\lam_2)\la\lam_1|\lam_2\ra=0 \implies \la\lam_1|\lam_2\ra=0.
\ee
Even if eigenvectors of the same eigenvalue are not orthogonal, we can make it orthogonal by Gram-Schmidt procedure. Finally we can make all eigenvectors of an Hermitian operator orthogonal, thus they can be used as a basis for the vector space.\\

Another inference is that if two operators $L$, $M$ has the same set of eigenvectors, they commute. We'll use the labels $\lam_i$ and $\m_\al$ to denote the eigenvalues of $L$ and $M$, and assume that there is a basis of state-vectors $|\lam_i,\m_\al\ra$ that are simultaneous eigenvectors of both observables.
\bs
L|\lam_i,\m_\al\ra&=\lam_i|\lam_i,\m_\al\ra\\
M|\lam_i,\m_\al\ra&=\m_\al|\lam_i,\m_\al\ra,
\end{split}\ee
the by acting on any of the basis vector, which we omit the subscript later, by the two operators,
\be
LM|\lam,\m\ra=\lam\m|\lam,\m\ra=ML|\lam_i,\m_\al\ra \implies [L,M]|\lam,\m\ra=0,
\ee
since any vector is composed of the basis vectors, $[L,M]$ acting on any vector gives zero,
\be [L,M]=0,\ee
thus they commute. It turns out that he converse of this theorem is also true: if two observables commute, then there is a complete basis of simultaneous eigenvectors of the two observables. Moreover, one may need to specify a larger number of observables to completely label a basis, regardless of the number of observables that are needed, they must all commute, we call this collection \textit{a complete set of commuting observables}.\\

2. The Uncertainty Principle\\

Suppose the eigenvalues of the observable $A$ is called $a$, then the expectation value of $A$ is
\be
\la A\ra= \sum_a aP(a).
\ee
Roughly speaking this means that $P(a)$ is centered around the expectation value. What we will mean by "the uncertainty in $A$ is the so-called \textit{standard deviation} $\bar A$, which is computed by subtracting from $A$ its expectation value,
\be
\bar A=A-\la A\ra=A-\la A\ra I.
\ee
The eigenvectors of $\bar A$ are the same as those of $A$ and the eigenvalues of $\bar A$ is shifted from that of $A$ by the average value $\la A\ra$,
\be
\bar A=a-\la A\ra.  
\ee
The square if the uncertainty of $A$ is defined by
\be
(\Del A)^2:=\sum_a \bar A^2 P(a)=\sum_a (a-\la A\ra)^2 P(a)=\la \Psi| \bar A^2|\Psi\ra,
\ee
if the expectation value of $A$ is zero, then the uncertainty takes the simpler form
\be
(\Del A)^2=\la \Psi| A^2|\Psi\ra.
\ee

If $\vec X$ and $\vec Y$ are two vectors, then the \textit{Triangle Inequality} gives
\be
|\vec X|+|\vec Y|\geqslant |\vec X+\vec Y|,
\ee
by squaring and sorting it we have
\be
|\vec X||\vec Y|\geqslant \vec X\cdot\vec Y,
\ee
and by squaring it again we have
\be
|\vec X|^2|\vec Y|^2\geqslant |\vec X\cdot\vec Y|^2,
\ee
which is called the \textit{Cauchy-Schwarz} inequality.

For complex vector spaces, the magnitude of the vectors are given by
\bs
|X|&=\sqrt{\la X|X\ra}\\
|X+Y|&=\sqrt{(\la X|+\la Y|)(|X\ra+|Y\ra)},
\end{split}\ee
squaring them and using the Triangle Inequality we have
\be
2|X||Y|\geqslant |\la X|Y\ra+\la Y|X\ra|.
\ee
Now we take $|X\ra$ and $|Y\ra$ as follows,
\bs
|X\ra&=A|\Psi\ra\\
|Y\ra&=iB|\Psi\ra,
\end{split}\ee
where $A$ and $B$ are any two observables and $|\Psi\ra$ is any ket. Now substitute it into the Triangle Inequality for complex vectors, to get
\be
2\sqrt{\la A^2\ra\la B^2\ra} \geqslant |\la \Psi|AB|\Psi\ra-\la \Psi|BA|\Psi\ra|=|\la \Psi|[A,B]|\Psi\ra|.
\ee
Let's suppose for the moment that $A$ and $B$ have expectation values of zero, so $A=\bar A$, and $\la A^2\ra=(\Del A)^2$, thus we rewrite the above inequality as 
\be
\Del A\Del B\geqslant \frac 1 2|\la \Psi|[A,B]|\Psi\ra|.
\ee
This is the general version of the uncertainty inequality. It says that the product of the uncertainties cannot be smaller than half the magnitude of the expectation value of the commutator. Or to be less quantitative, if the commutator of $A$ and $B$ is not zero, then both observables cannot simultaneously be certain. By the way, if the expectation values of $A$ and $B$ are not zero, we just use a trick to redefine two new operators in which the expectation values have been subtracted off, then the same relation holds as we expected.\\

Later we will come to a specific version of the inequality --- \textit{Heisenberg's Uncertainty Principle}: The product of the uncertainties of the position and momentum of a particle cannot be less than half of Plank's constant.


\subsection{Spin states, spin operators, and Pauli Matrices}
\textit{All possible spin states can be represented in a two dimensional vector space.}

\be
|A\ra=\al_u|u\ra+\al_k|d\ra,
\ee
where the $u$ and $d$ represents for pointing up and down when we measure spin along z-axis, and the orthogonality means if we get up we never get down, vice versa.\\
Now we choose A to be pointing left and right, considering that they're orthonormal, and it is equally likely to be up and down when measured along z, we write them as
\be\1\{\begin{split}
|r\ra&=\frac1 {\sqrt{2}}|u\ra+\frac1 {\sqrt{2}}|d\ra\\
|l\ra&=\frac1 {\sqrt{2}}|u\ra- \frac1 {\sqrt{2}}|d\ra.
\end{split}\2.\ee
And when we represent forward and backward, we must further take into account the equal likelihood of left and right when measured along x-axis, so we take the following form,
\be\1\{\begin{split}
|f\ra&=\frac1 {\sqrt{2}}|u\ra+\frac i {\sqrt{2}}|d\ra\\
|b\ra&=\frac1 {\sqrt{2}}|u\ra- \frac i {\sqrt{2}}|d\ra.
\end{split}\2.\ee
Notice that all the above choices have the same freedom by a phase factor $e^{i\theta}$.

We will use $\sig$ to denote spin operator. Measuring along z-axis gives us
\be\1\{\begin{split}
\sig_z |u\ra&=|u\ra\\
\sig_z |d\ra&=-|d\ra,
\end{split}\2.\ee
if we denote $|u\ra$ and $|d\ra$ as unit vectors as $\1(\ba{c}1\\0\ea\2)$ and $\1(\ba{c}0\\1\ea\2)$, we may figure out that
\be
\sig_z=\1(\ba{cc}1&0\\0&-1\ea\2).
\ee
Measuring along x-axis gives us
\be\1\{\begin{split}
\sig_x |r\ra&=|r\ra\\
\sig_x |l\ra&=-|l\ra,
\end{split}\2.\ee
if we denote $|r\ra$ and $|l\ra$ as unit vectors as $\1(\ba{c}\frac1{\sqrt{2}}\\\frac1{\sqrt{2}}\ea\2)$ and $\1(\ba{c}\frac1{\sqrt{2}}\\-\frac1{\sqrt{2}}\ea\2)$, we may figure out that
\be
\sig_x=\1(\ba{cc}0&1\\1&0\ea\2).
\ee
Measuring along y-axis gives us
\be\1\{\begin{split}
\sig_y |f\ra&=|f\ra\\
\sig_y |b\ra&=-|b\ra,
\end{split}\2.\ee
if we denote $|f\ra$ and $|b\ra$ as unit vectors as $\1(\ba{c}\frac1{\sqrt{2}}\\\frac i{\sqrt{2}}\ea\2)$ and $\1(\ba{c}\frac1{\sqrt{2}}\\-\frac i{\sqrt{2}}\ea\2)$, we may figure out that
\be
\sig_y=\1(\ba{cc}0&-i\\i&0\ea\2).
\ee
The matrix representations of $\sig_x$, $\sig_y$ and $\sig_z$ are called the \textit{Pauli Matrices}.\\
It is easy and useful to verify the commutation relations of components of spins,
\be
[\sig_j,\sig_k]=2i\ep_{jkl}\sig_l.
\ee

Now we may treat $\sig_x$, $\sig_y$ and $\sig_z$ as three components of $\sig$, thus $\sig$ can be viewed as a general vector, and it can be projected onto a unit vector $\hat n$ point any direction in space, so as to measure spin along that direction.
\be
\sig_n=\vec\sig\cdot\hat n=\sig_xn_x+\sig_yn_y+\sig_zn_z=\1(\ba{cc}n_z&n_x-in_y\\n_x+in_y&-n_z\ea\2),
\ee
from the fact that
\bs
tr(\sig_n)&=0\\
det(\sig_n)&=-1,
\end{split}\ee
we could immediately figure out that the two eigenvalues of $\sig_n$ are give by $\pm 1$. And we could work out its eigenvectors in a certain configuration, if $\hat n$ lies x-z plane, we may write
\be\1\{\begin{split}
n_z&=\cos\theta\\
n_x&=\sin\theta\\
n_y&=0,
\end{split}\2.\ee
thus 
\be
\sig_n=\1(\ba{cc}\cos\theta&\sin\theta\\\sin\theta&-\cos\theta\ea\2),
\ee
so the eigenvectors correspond to each eigenvalues are given by
\be\1\{\begin{split}
\lam_1=1, &\quad|\lam_1\ra=\1(\ba{c}\cos{\frac\theta 2}\\\sin{\frac\theta 2}\ea\2)\\
\lam_2=-1, &\quad|\lam_1\ra=\1(\ba{c}-\sin{\frac\theta 2}\\\cos{\frac\theta 2}\ea\2).
\end{split}\2.\ee
Now suppose we prepare a $|u\ra$ state, and we measure the probabilities of $+1$ and $-1$ along the $\hat n$ direction, we get
\be\1\{\begin{split}
P(+1)=&\la u|\lam_1\ra\la\lam_1|u\ra=\cos^2{\frac{\theta}{2}}\\
P(-1)=&\la u|\lam_2\ra\la\lam_2|u\ra=\sin^2{\frac{\theta}{2}}
\end{split}\2.\ee
Now we're ready to compute the average value of spin along the $n$ direction,
\be
\la \sig_n\ra=\sum_j \lam_j P(\lam_j)=\cos^2{\frac{\theta}{2}}-\sin^2{\frac{\theta}{2}}=\cos{\theta},
\ee
which agrees perfectly with our expectation.\\

To proceed, we come to an important theorem,\\

\textit{\textbf{The Spin-Polarization Principle:} Any state of a single spin is an eigenvector of some component of the spin.}\\

In other words, given any state
\be
|A\ra=\al_u|u\ra+\al_d|d\ra,
\ee
there exists some direction $\hat n$, such that
\be
\vec\sig\cdot\vec n|A\ra= |A\ra.
\ee
In physics language, we say that the states of a spin are characterized by a \textit{polarization vector}, and along that polarization vector the component of spin is predictably $+1$, assuming of course that you know the state-vector.\\ It follows that the expectation value of the spin along the direction $\hat n$ can be expressed as
\be
\la\sig_n\ra=\la \vec\sig\cdot\vec n\ra=1.
\ee\\

In fact, any observable of a spin is represented by a $2\times2$ Hermitian matrix, and has the form
\be
\1(\ba{cc}r&w\\w^*&r'\ea\2),
\ee
the implication is that it takes exactly four real parameters to specify this observable. And, there is a neat way to write any spin observable in terms of the Pauli matrices, $\sig_x$, $\sig_y$, $\sig_z$, and the unit matrix $I$,
\be S=a I+b_1\sig_x+b_2\sig_y+b_3\sig_z=a I+\vec b\cdot\vec \sig, \ee
with $a$ and $b_i$ real numbers.

\subsection{States and Operators in Continuous basis}
1. States and the inner product\\

A particle at a fixed position $x_0$ (in one dimension) can be represented by a state vector $|x_0\ra$. In general, a state is a superposition of various position state, since position is continuous variable, we must turn the sum into integral,
\be
|\psi\ra=\int \dif x \ \psi(x)|x\ra,
\ee
where the coefficient, or wave function $\psi(x)$ is computed as
\be
\psi(x)=\la x|\psi\ra,
\ee
substituting the coefficient into the decomposition we have
\be
|\psi\ra=\int \dif x \ |x\ra \la x|\psi\ra,
\ee
from which we find an identity as
\be
I=\int \dif x \ |x\ra \la x|,
\ee
which implies summing over all projections $|x\ra \la x|$, we can recover the original state.\\
Left multiplying the decomposition of state by a bra vector $|x'\ra$, we have
\be
\psi(x')=\la x'|\psi\ra=\int \dif x \ \psi(x)\ \la x'|x\ra,
\ee
from which we can see the inner product between basis is nothing but the Dirac-delta function,
\be
\la x'|x\ra=\delta(x-x'),
\ee
this is the orthonormalization relation of the basis vectors.\\

Using the above relations we may naturally find the expression of inner product of states in the continuous position basis, just by inserting two different sets of the identity relations of $x$ and $x'$,
\be
\la \phi|\psi\ra = \int \dif x \int \dif x' \ \la \phi|x'\ra\la x'|x\ra\la x|\psi\ra=\int \dif x \int \dif x' \phi^*(x') \del(x'-x) \psi(x)=\int \dif x \ \phi^*(x)\psi(x),
\ee
thus the inner product of a state with itself is given by
\be
\la \psi|\psi\ra =\int \dif x \ \psi^*(x)\psi(x),
\ee
this will remind us of the expression of the probability, and due to the continuous property we will denote our corresponding concept as the "probability density", which is given by
\be
P(x):=\psi^*(x)\psi(x),
\ee
and the probability of state in an interval $[a,b]$ is given by
\be
P([a,b])=\int_a^b \dif x\ P(x)=\int_a^b \dif x\ \psi^*(x)\psi(x),
\ee
and because the total probability equal to $1$, our state here is normalized,
\be
\la \psi|\psi\ra =\int \dif x \ \psi^*(x)\psi(x)=\int  \dif x\ P(x)=1.\\
\ee
2. Linear Operators $X$ and $P$, their eigenvectors, eigenvalues and relations\\

An operator is a machine which can turn a given state into a new state, in terms of the continuous basis, if the coefficients (or the wave function) of the new state is specified, the new state would be determined, thus the effect of the operator is made clear.

Let's set an example. We want to define an operator $X$ which can turn a state $|\psi\ra$ in to a new state $|\bar\psi\ra$, now the state $|\psi\ra$ is given, so the definition of $X$ is clear only if we know what the new state $|\bar\psi\ra$ is. By ``the state $|\psi\ra$ is given", we mean in terms of any basis, we know exactly what the coefficients of the state $|\psi\ra$ are, here we choose the continuous position basis, where we know exactly the wave function $\psi(x)$. So the task now shifts from defining $X$ to giving the expression of $\bar\psi(x)$, and we define it to be $\bar\psi(x)=x\psi(x)$, thus we say we complete the definition of the operator $X$.

By mathematical expressions, we write the definition of $X$ as
\be
X|\psi\ra=|\bar\psi\ra \quad\text{s.t.}\quad \bar\psi(x)=x\psi(x),
\ee
or in a more neat form,
\be
\la x|X|\psi\ra:=x\la x|\psi\ra.
\ee
Here we say that $x$ is a \textit{basis-dependent operator corresponding to $X$}. Note that an operator only acts on a state in the abstract vector space, while a basis-dependent operator acts on the wave function. Due to the fact that both the abstract state vector and the wave function can be multiplied by a number, sometimes we blur the distinction of the two concept and just write $X=x$, but this is not generally true. The clear relation between $X$ and $x$ can be seen by inserting an identity relation to the definition, as
\be
\int \dif x'\ X_{xx'} \psi(x')=\int \dif x' \ \la x|X|x'\ra\la x'|\psi\ra=\la x|X|\psi\ra=x\la x|\psi\ra=x \psi(x),
\ee
and by observing we have 
\be\1\{\begin{split}
&X_{xx'}= \la x|X|x'\ra=x\del(x-x')\\
&x=\int \dif x'\ X_{xx'}=\int \dif x'\ \la x|X|x'\ra,
\end{split}\2.\ee
which clearly shows in position basis $X$ is a matrix with only non-zero diagonalized elements $x\del(x-x')$, and $x$ is the sum of the elements of the $x'$th column.\\

Then by bearing the above in mind, we proceed to define a new operator $D$ as
\be
\la x|D|\psi\ra:=\frac\p{\p x}\la x|\psi\ra,
\ee
where the partial derivative $\frac\p{\p x}$ is the basis-dependent operator corresponding to $D$. Notice that now we CANNOT write $D=\frac\p{\p x}$, because $\frac\p{\p x}|\psi\ra$ is meaningless due to the basis-independence of $|\psi\ra$. The relation between $D$ and $\frac\p{\p x}$ can be seen by the similar process as in the case of $X$ and $x$, and here we directly list the results:
\be\1\{\begin{split}
D_{xx'}&= \la x|D|x'\ra=\del(x-x')\frac\p{\p x}\\
\frac\p{\p x}&=\int \dif x'\ D_{xx'}=\int \dif x'\ \la x|D|x'\ra.
\end{split}\2.\ee\\

Operators with physical interests are Hermitian operators. A convenient way to judge the hermitian property is by using the inner product. The inner product is number, assuming $O$ is hermitian, we take the complex conjugate of the inner product,
\be
\la\phi|O|\psi\ra^*=\la\psi|O^\dagger|\phi\ra=\la\psi|O|\phi\ra,
\ee
finding that switching the left and right vector gives the complex conjugate of the original, and this will serve as the criteria for judging the hermitian property in the following.

So are $X$ and $D$ hermitian? We first deal with $X$,
\be\begin{split}
\la\phi|X|\psi\ra^*&=\1[\int\dif x\ \la\phi|x\ra\la x|X|\psi\ra\2]^*=\1[\int\dif x\ \phi^*(x)\ x \psi(x)\2]^*\\
&=\int\dif x\ \psi^*(x)\ x^* \phi(x)=\int\dif x\ \psi^*(x)\ x \phi(x)\\
&=\la\psi|X|\phi\ra,
\end{split}\ee
where we used the fact that the position variable is real. Thus the position operator is indeed Hermitian.

And next the $D$ operator,
\be\begin{split}
\la\phi|D|\psi\ra^*&=\1[\int\dif x\ \la\phi|x\ra\la x|D|\psi\ra\2]^*=\1[\int\dif x\ \phi^*(x)\  \frac\p{\p x}\psi(x)\2]^*\\
&=\1[\cancel{\int_{-\infty}^{\infty} \dif x\ \psi^*(x)\ \phi(x)}-\int\dif x\ \frac\p{\p x}\psi^*(x)\ \phi(x)\2]^*=-\int\dif x\ \phi^*(x)\  \frac\p{\p x}\psi(x)\\
&=-\la\psi|D|\phi\ra \implies D^\dagger=-D,
\end{split}\ee
where we have used the boundary condition that the wave functions vanish at infinity. Thus the $D$ operator is not Hermitian, but anti-Hermitian. But actually we still want to construct an Hermitian operator out of $D$, recall that the easiest way to construct an Hermitian operator out of an anti-Hermitian operator is multiplying it by $i$ or $-i$. So we define a new operator as
\be
P:=-i\hbar D,
\ee
and it's easy to check that $P$ is Hermitian,
\be
\la\phi|P|\psi\ra^*=[-i\hbar\la\phi|D|\psi\ra]^*=i\hbar[-\la\psi|D|\phi\ra]=\la\psi|P|\phi\ra,
\ee
and its basis-dependent counterpart is naturally given by $\hat p=-i\hbar\frac\p{\p x}$, which can act directly on the wave function,
\be
\hat p\psi(x)=-i\hbar\frac\p{\p x}\psi(x).
\ee\\

Next we explore the eigenvalues and eigenvectors of $X$ and $P$. Eigenvectors of an operator are, by definitions, vectors that acted by the operator gives the same vector multiplied by a number which is called the eigenvalue. For the $X$ operator, we are looking for a solution that satisfies,
\be
X|\psi\ra=x_0|\psi\ra,
\ee
where $x_0$ is a specific number. By definition, the LHS in $x$-basis is given by
\be
\la x|X|\psi\ra=x\la x|\psi\ra=x\psi(x),
\ee
and the RHS in the $x$-basis would be
\be
\la x|x_0|\psi\ra=x_0\la x|\psi\ra=x_0\psi(x).
\ee
So the equation we're going to solve is nothing but
\be
x\psi(x)=x_0\psi(x),
\ee
and when obtaining the expression for $\psi(x)$, we can easily recover the state vector by the superposition principle,$|\psi\ra=\int\dif x\ \psi(x)|x\ra$.\\
By rearranging the equation to solve, we get
\be
(x-x_0)\psi(x)=0,
\ee
which says that $\psi(x)$ could be non-zero only when $x=x_0$, or equivalently, when $x\ne x_0$, $\psi(x)$ must be zero. We summarize this as follows,
\be
\psi(x)= \begin{cases}
\text{(non-)zero}, & x=x_0 \\
0, & x\ne x_0,
\end{cases}
\ee
this reminds us of the Dirac-delta function $\delta(x-x_0)$, but we didn't strictly prove that $\psi(x)$ is indeed $\del(x-x_0)$. Since a straightforward proof is a little troublesome and will lead to some ambiguities (as I tried), we go around that and try to find another clever way. As all we want is a function that satisfies $x\psi(x)=x_0\psi(x)$, if we somehow happen to find such a function, we can say that it is the eigenfunction of the operator. The trick here is to use the relation between $X$ and $x$, as follows,
\be
x_0\del(x-x_0)=[x_0\del(x_0-x)]^*=\la x_0|X|x\ra^*=\la x|X|x_0\ra=x\del(x-x_0),
\ee
where we have used the Hermitian property of $X$, the realness of $x_0$ and $\del(x_0-x)$, and the evenness of $\del(x_0-x)$. Now we see that the Dirac-delta function $\del(x-x_0)$ indeed is a solution to the equation, so it is the eigenfunction, $\psi(x)=\delta(x-x_0)$, now we recover our eigenvector as
\be
|\psi\ra=\int\dif x\ \psi(x)|x\ra=\int\dif x\ \del(x-x_0) |x\ra =|x_0\ra,
\ee
and finally our eigen-equation will be
\be
X|x_0\ra=x_0|x_0\ra.
\ee
Since $x_0$ can be any value along the $x$-axis, this equation can be generalized to
\be
X|x\ra=x|x\ra,
\ee
which tells us the simple fact that any state representing a definite position is an eigenvector of the operator $X$, and the eigenvalue is the position coordinate; thus the operator $X$ is called the position operator, and the wave function in the position representation $\psi(x)$ is the projection of the state-vector onto the eigenvectors of position.\\

Then we play around with $P$. Similarly the eigen-equation is given by
\be
P|\psi\ra=p_0|\psi\ra,
\ee
and in $x$-basis,
\be
p_0\psi(x)=p_0\la x|\psi\ra=\la x|p_0|\psi\ra=\la x|P|\psi\ra=-i\hbar\frac\dif{\dif x}\la x|\psi\ra=-i\hbar\frac\dif{\dif x}\psi(x),
\ee
which is nothing but a simple first-order equation. The solution is given by
\be
\psi(x)=Ae^{\frac i \hbar p_0 x},
\ee
where $A$ will be normalized as $\frac 1 {\sqrt{2\pi}}$. The eigenvector is constructed as
\be
|\psi\ra=\int\dif x\ \psi(x)|x\ra=\int\dif x\ e^{\frac i \hbar p_0 x} |x\ra \equiv|p_0\ra,
\ee
which is just the Fourier-transformation of the state $|x\ra$. So the eigen-equation will be recovered as
\be
P|p_0\ra=p_0|p_0\ra \quad\text{or}\quad P|p\ra=p|p\ra,
\ee
where the physical meaning of $p$ and the operator $P$ will be clear later when we take the classical limit, but actually you may already guess that they are momentum operators. As an aside, here we get a commonly used relation by re-express the solution,
\be
\la x|p\ra=\la x|\psi\ra=\psi(x)=\frac 1 {\sqrt{2\pi}}e^{\frac i \hbar p x}, \quad\text{and}\quad \la p|x\ra=\frac 1 {\sqrt{2\pi}}e^{-\frac i \hbar p x},
\ee
where the left equation represents the a momentum eigenfunction in the position basis, with $p$ fixed and $x$ changeable; the right equation represents the converse case.

To this end, we really see that the eigenfunction of the $P$ operator in the position basis has the form of a wave, and that's why we call it a ``wave function". Moreover, the wave length is given by
\be
\lambda=\frac{2\pi\hbar}{p},
\ee
which is the famous de Broglie's relation between the momentum and the wavelength. An implication of this formula is the fact that to probe objects of ever smaller size one needs particles of ever larger momentum, that's why we need increasingly powerful particle accelerators.\\

Now that we have an Hermitian operator $P$ and its eigenvectors $|p\ra$, we can decompose the state vector $|\psi\ra$ in this basis, just parallel to do it in the position basis,
\be
|\psi\ra=\int \dif x \ \tilde\psi(p)|p\ra,\quad\text{where}\quad \tilde\psi(p)=\la p|\psi\ra,
\ee
and the identity is subtracted as
\be
|\psi\ra=\int \dif p \ |p\ra \la p|\psi\ra \implies I=\int \dif p \ |p\ra \la p|.
\ee
Next we want to find the relation between $\tilde\psi(p)$ and $\psi(x)$, that is to say, if we knew one of them, we could calculate the other, so as to know the probability of the given interval of the other, e.g, in $p$-basis, $P(p)=\tilde\psi^*(p)\tilde\psi(p)$.\\
We simultaneously solve the relation using the Dirac's elegant bra-and-ket's notation,
\be\left\{\begin{split}
&\tilde\psi(p)=\la p|\psi\ra=\int\dif x\ \la p|x\ra \la x|\psi\ra=\frac 1 {\sqrt{2\pi}}\int\dif x\ e^{-\frac i\hbar p x} \psi(x)\\
&\psi(x)=\la x|\psi\ra=\int\dif p\ \la x|p\ra \la p|\psi\ra=\frac 1 {\sqrt{2\pi}}\int\dif p\ e^{\frac i\hbar p x} \psi(p),
\end{split}\right.\ee
which are reciprocal \textit{Fourier transformations} of one another.\\

Now that we know the exact form of $X$ and $P$, we may find their commutator and hence we know the uncertainty principle of them. Their commutator is computed as,
\be\begin{split}
\la x |[X,P]|\psi\ra=\la x|XP|\psi\ra-\la x|PX|\psi\ra&=\int\dif x'\ \la x|X|x'\ra\la x'|P|\psi\ra-\int\dif x'\ \la x|P|x'\ra\la x'|X|\psi\ra\\
&= -i\hbar\int\dif x'\ x'\delta(x'-x) \frac\dif{\dif x'} \psi(x')+i\hbar\int\dif x'\ \delta(x'-x)\frac\dif{\dif x} \big(x'\psi(x')\big)\\
&=i\hbar\1(\frac\dif{\dif x} \bigg(x\psi(x)\bigg)-x\frac\dif{\dif x} \psi(x)\2)\\
&=i\hbar\psi(x)=\la x|i\hbar|\psi\ra,
\end{split}\ee
thus we find the commutator is nothing but a complex number,
\be
[X,P]=i\hbar.
\ee
And according to the uncertainty principle,
\be
\Del A\Del B\geqslant \frac 1 2|\la \Psi|[A,B]|\Psi\ra|,
\ee
we have the uncertainty between $X$ and $P$,
\be
\Del X\Del P\geqslant \frac 1 2|\la \Psi|[X,P]|\Psi\ra|=\frac 1 2 |i\hbar\la\psi|\psi\ra|=\frac \hbar 2,
\ee
this is the famous \textit{Heisenberg's uncertainty principle}, which means you cannot measure the position and $P$ in the same time, with $\frac \hbar2 $ the limitation.

As a qualitative illustration of the Heisenberg's uncertainty principle, we knew previously the wave function of an eigenstate of $X$ is highly concentrated about some point $x_0$, the probability is perfectly localized, and so is true for the eigenstate of $P$. But what about the probability of $P$ eigenstate in the $X$-basis? It's just
\be
\psi_p^*(x)\psi_p(x)=\la p|x\ra\la x|p\ra = \1(\frac 1 {\sqrt{2\pi}}e^{-\frac i \hbar p x}\2)\1(\frac 1 {\sqrt{2\pi}}e^{\frac i \hbar p x}\2)=\frac 1 {2\pi}.
\ee
The result is completely uniform, with no peaks anywhere on the $x$ axis. Evidently, a state with definite $p_0$ is completely uncertain in its position, vice versa.






\newpage
\section{Symmetry Operations of Quantum States and Conserved Quantities}
\subsection{Time evolution of isolated states and the Hamiltonian}

In classical mechanics, if two identical isolated systems start out in different states, they stay in different states. It underlies the fact that information is never lost. The same is true in quantum mechanics, and even much stronger. The direct manifestation is that if two states are initially orthogonal, they remain orthogonal throughout the evolution, or \textit{conservation of orthogonality}.

We denote a state at time $t$ as $|\Psi(t)\ra$, which originates from the state at time $t=0$, $|\Psi(0)\ra$, by acting an \textit{time-evolution operator} $\hat U(t)$ on it,
\be
|\Psi(t)\ra=\hat U(t) |\Psi(0)\ra.
\ee
This equation reflects the fact that quantum evolution of states allows us to compute the probabilities of the outcomes of later experiments, in contract to the case in classical mechanics, classical determinism allows us to predict the results of experiments. 

Mathematically, the conservation of orthogonality tells us
\be
\la \Psi(0)|\Phi(0)\ra=0 \implies \la \Psi(t)|\Phi(t)\ra=\la \Psi(0)|\hat U^\dagger(t) \hat U(t)|\Phi(0)\ra=0,
\ee
if we choose $|\Psi(0)\ra$ and $|\Phi(0)\ra$ to be any two orthonormal basis of vectors $|i\ra$ and $|j\ra$, we could obtain a property of the evolution operator,
\be
\la i|j\ra =\delta_{ij} = \la i|\hat U^\dagger(t) \hat U(t)|j\ra \implies \hat U^\dagger(t) \hat U(t)=I,
\ee
with which the evolution operator satisfies is called \textit{unitary}.\\

\textit{Principle: The evolution of state-vectors with time is unitary.}\\

A direct consequence of the unitarity leads to a much stronger version of conservation of orthogonality --- \textit{conservation of overlaps},
\be
\la A(t)|B(t)\ra = \la A(0) |\hat U^\dagger(t)\hat U(t)|B(0)\ra = \la A(0)|B(0)\ra,
\ee
which says the inner product of $|A\ra$ and $|B\ra$ does not change with time, or the inner product is conserved. Thus superficially conservation of orthogonality is a special case of conservation of overlap, we have seen that conservation of orthogonality can lead to conservation of overlap, thus they are equivalent.\\

Time evolution is led by incremental changes, so it would be inspiring to study the evolution operator of an infinitesimal time interval $\epsilon$. Another property besides unitarity that an evolution operator should have is \textit{continuity}, which means that the state-vector changes smoothly.

When $\epsilon$ is very small, $\hat U(\epsilon)$ is close to the unit operator, only differing from it by something of order $\epsilon$, and when $\ep$ is taken to be zero, the state does not change. 

Or to be clearer, the most general form of a state obtained by acting the infinitesimal evolution operator on the initial state, considering continuity, takes the form as
\be
U(\ep)|\Psi\ra=|\Psi\ra+\ep|\Theta\ra,
\ee
where $|\Theta\ra$ is some unknown state though, it can be obtained by acting some operator to the original state $|\Theta\ra\equiv A|\Psi\ra$, so we have
\be
U(\ep)|\Psi\ra=|\Psi\ra+\ep A|\Psi\ra =(I+\ep A)|\Psi\ra,
\ee
then we have a form of $U(\ep)$ as
\be
U(\epsilon)=I+\ep A,
\ee
where the property and meaning of $A$ is to be explored. Using the unitarity, we have
\be
I=U^\dagger(\epsilon)U(\epsilon)=(I+\ep A^\dagger)(I+\ep A)=I+\epsilon (A^\dagger+A),
\ee
or
\be
A^\dagger=-A,
\ee
which says $A$ is anti-Hermitian.\\
But it is easy to construct an Hermitian operator, which is more desired by us, out of the anti-Hermitian operator just by multiplying the imaginary unit on it, ie, 
\be\ma H\equiv iA =(-i)(-A)= (iA)^\dagger = \ma H^\dagger,\ee
thus we have the form of the infinitesimal evolution operator as
\be
U(\epsilon)=I+\ep A=I+\epsilon (-i)(iA)=1-i\ep\ma H,
\ee
and due to the Hermitian property of $H$, it has the great significance that $H$ is an observable and has a complete set of orthonormal eigenvalues and eigenvectors.\\

Now we're going to explore more on the evolution of states at an infinitesimal time interval $\epsilon$, we write
\be
|\Psi(\ep)\ra=\hat U(\ep) |\Psi(0)\ra = |\Psi(0)\ra-i\ep\ma H|\Psi(0)\ra,
\ee
and rearranging this we have
\be
\frac{|\Psi(\ep)\ra-|\Psi(0)\ra}{\ep}=-i\ma H|\Psi(0)\ra,
\ee
where the LHS is nothing but the time derivative of the state $|\Psi\ra$ at time $t=0$, but since the evolution is assumed to be linear, this should hold at any time $t$, so we may write
\be
\frac{\dif |\Psi\ra}{\dif t}=-i\ma H|\Psi\ra,
\ee
where we wrote partial or total derivative make no difference. This equation tells us how an isolated (not contact with apparatus) state evolves with time, and it is called the \textit{generalized Schrodinger equation}. This is a simple first-order differential equation, and its solution is given by
\be
|\Psi(t)\ra=e^{-i\ma Ht}|\Psi(0)\ra,
\ee
thus we may extract the explicit form of the evolution operator as $U(t)=e^{-iHt}$, and we check it by taking the time to be infinitesimal time interval $\ep$,
\be
U(\ep)=e^{-i\ma H\ep}\approx 1-i\ma H\ep,
\ee
which is consistent with our previous reasoning. From this exponential form we now know why we call it unitary, and a unitary operator has another property that its eigenvalues are eigenvalues of the phase operator, in this case, $\ma H$.

Next we want to compare it with classical mechanics to see the meaning of $\ma H$ clearly. The average, or expectation value, of an observable is the closest thing in quantum mechanics to a classical value. From a mathematical point of view, an average is defined by the equation
\be
\la\lam \ra = \sum_i \lam_i P(\lam_i).
\ee
If we expand a quantum state $|\Psi\ra$ in the orthonormal basis of eigenvectors of $L$,
\be
|\Psi\ra=\sum_i\al_i|\lam_i\ra,
\ee
we try to compute the following quantity,
\be
\la L \ra\equiv\la \Psi|L|\Psi\ra=\sum_i\sum_{j}\la \lam_j| \al^*_j \lam_i \al_i|\lam_i\ra=\sum_i  \lam_i (\al^*_i\al_i)= \sum_i \lam_i P(\lam_i)=\la \lam \ra,
\ee
that's why we use the notation $\la L\ra$ to denote the average. Let's see how this changes with time,
\bs
\frac\dif{\dif t}\la L \ra=\frac\dif{\dif t}\la \Psi(t)|L|\Psi(t)\ra&=\la \dot\Psi(t)|L|\Psi(t)\ra+\la \Psi(t)|L|\dot\Psi(t)\ra\\
&= i  \la \Psi(t)|(\ma HL-L\ma H)|\Psi(t)\ra= i \la [\ma H,L]\ra,
\end{split}\ee
where in the second row we used the generalized Schrodinger equation, and the notation for a commutator. Note that the $i$ before the commutator is important, since the commutator itself is always imaginary.\\
Equivalently we rewrite it in a shorthand form:
\be\label{dl}
\frac{\dif L}{\dif t}=- i  [L,\ma H],
\ee
where letters denote their average value. This is a very interesting and important equation. It relates the time derivative of the expectation value of an observable $L$ to the expectation value of another observable $[L,\ma H]$. If we assume that the probabilities are nice, narrow, bell-shaped curves, this equation tells us how the peaks of the curves move with time. Equations like this are the closest thing in quantum mechanics to the equations of classical physics.\\

The equation tells us the expectation value of an observable $\la Q\ra$  does not change with time if it commutes with the $\ma H$,
\be
\frac{\dif\bar Q}{\dif t}=-\frac i \hbar\overline{[Q,\ma H]} =0.
\ee
Moreover if $Q$ commutes with the Hamiltonian, the expectation values of all powers of $Q$, and even all functions of $Q$ are conserved.\\
As a special case, the most obvious conserved quantity is $\ma H$ itself, since it commutes with itself,
\be
\frac{\dif\bar{\ma H}}{\dif t}=-i \overline{[\ma H,\ma H]}=0,
\ee
which says it is a conserved quantity, and we define it to be the energy, since it is conserved in the most general state, considering only the unitarity and continuity of the evolution operator, regardless of any vector basis or specific physical situations.

The only thing to be noticed is the fact that energy has the dimension of energy, by checking the dimensionality of the time-dependent Schrodinger equation, we may fix $\ma H$ by a factor of $\hbar$, the Planck constant, i.e, to define the true Hamiltonian operator as $H:=\hbar \ma H$, and we have
\be
i\hbar\frac{\dif |\Psi\ra}{\dif t}= H|\Psi\ra.
\ee
This is the standard form of the \textit{time-dependent Schrodinger equation}.\\

Recall that in classical mechanics, the time derivative of a physical quantity is given by its Poisson Bracket with the Hamiltonian,
\be
\frac{\dif L}{\dif t}=\{L, H\}.
\ee
Due to their similarity, we may identify a correspondence between the commutators and the Poisson Brackets,
\be
[L,H] \iff  i \hbar\{L, H\}.
\ee
{\color{blue}Actually they have the same mathematical structures (like anti-symmetric, Bianchi identity) called the Lie Algebra, which would be explored later if we have time.}\\
In classical physics, commutators between ordinary observables are zero. But if we want to apply \eqref{dl} to classical physics, we must assume the commutators are not zero but a very small value of the order of $\hbar$. Vice versa, the classical limit is the limit at which $\hbar$ is negligibly small, thus the $i\hbar\{L,H\}$ goes back to the vanishing commutator.\\

Now it's more concrete to set the spin as an example. The energy of a spin in a magnetic field is given by
\be
H\sim \vec\sig\cdot\vec B=\sig_x B_x+\sig_y B_y+\sig_z B_z.
\ee
If the magnetic field lies along the z-axis, the Hamiltonian is proportional to $\sig_z$, in the following expression we'll absorb all the irrelevant numerical constants into a single constant $\omega$, so
\be 
H=\frac{\hbar\omega}{2} \sig_z,
\ee
where we kept $\frac\hbar 2$ for later convenience.\\
Now we want to find out how the expectation value of the spin varies with time, we can get
\be\begin{split}
\dot{\la\sig_x\ra}&=-\frac i \hbar \la[\sig_x,H]\ra=-\frac{i\omega} 2\la[\sig_x,\sig_z]\ra=-\om\la\sig_y\ra\\
\dot{\la\sig_y\ra}&=-\frac i \hbar \la[\sig_y,H]\ra=-\frac{i\omega} 2\la[\sig_y,\sig_z]\ra=\om\la\sig_x\ra\\
\dot{\la\sig_z\ra}&=-\frac i \hbar \la[\sig_z,H]\ra=-\frac{i\omega} 2\la[\sig_z,\sig_z]\ra=0
\end{split}\ee
This result implies that the 3-vector-operator $\vec\sig$ precesses like a gyroscope around the direction of the magnetic field. The precession is uniform, with angular velocity $\om$. To be specific, the expectation value for a $\sig_z$ measurement does not change with time, but the other two expectation values do change.\\

Now that we have defined that $H$ is a Hermitian operator, we can expand our states in the basis of $H$'s eigenvectors, to fully solve the Schrodinger equation,
\be
|\Psi\ra=\sum_j\al_j|E_j\ra,
\ee
where $|E_i\ra$ satisfies
\be
H|E_i\ra=E_i|E_i\ra,
\ee
and $E_i$s are the eigenvalues of $H$, or energy.\\
Using the time-dependent Schrodinger equation and due the fact that basis vector do not change with time, we have
\be
\sum_j\dot\al_j(t)|E_j\ra=-\frac i \hbar H\sum_j\al_j(t)|E_j\ra=-\frac i \hbar \sum_jE_j\al_j(t)|E_j\ra,
\ee
or regrouping
\be
\sum_j\bigg\{\dot\al_j(t)+\frac i \hbar E_j\al_j\bigg\}|E_j\ra=0,
\ee
thus every coefficient must be zero, and we have again the simplest differential equation,
\be\frac{\dif \al_j(t)}{\dif t}=-\frac i \hbar E_j \al_j(t)\ee,
and the solution is given by
\be\al_j(t)=\al_j(0)e^{-\frac i\hbar E_j t}\ee.
This equation contains the underlying assumption that Hamiltonian does not depend explicitly on time. And it's the first time for us to see the deep connection between energy and time.
We may extract out coefficients at time zero as \be\al_j(0)=\la E_j|\Psi(0)\ra\ee, thus we can write the full solution fo the time-dependent Schrodinger equation as
\be
|\Psi(t)\ra=\sum_j\al_j(0)e^{-\frac i\hbar E_jt}|E_j\ra=\sum_j|E_j\ra\la E_j|\Psi(0)\ra e^{-\frac i\hbar E_jt},
\ee
which emphasizes that we're summing over the basis vectors, and implies that as long as we have the initial state $|\Psi(0)\ra$ and the Hamiltonian $H$ which governs the physical law of evolution, we can have state $|\Psi(t)\ra$ at any time as we want.\\

All the above formalism in this subsection describes how state evolves with time during two measurements. But something different happens when an observation is made, during an experiment the state of a system jumps unpredictably to an eigenstate of the observable that was measured. This phenomenon is called \textit{the collapse of the wave function}. This implies that to examine the measurement process itself as a quantum mechanical evolution, we must consider the entire experimental setup, including the apparatus, as part of a single quantum system.

\subsection{General symmetry transformations of states and wave functions}
Time evolution is a special case of symmetry transformations. Symmetry transformations are operations that you can do to the system, which don't change the phenomenon or the description of it, e.g, EOM. In this subsection, we are going to discuss more general properties of symmetry transformations and some specific cases like space translations and rotations.\\

A general symmetry transformation operator $V$ has two properties. The first is that it preserves orthogonality, which gives it the property of unitarity,
\be
\la \phi|V^\dagger V|\psi\ra=\la \phi|\psi\ra \ \implies\ V^\dagger V=I.
\ee
The second is that it preserves time-evolution, which means that a state $|\psi(0)\ra$ and its transformed state $|\psi'(0)\ra=V|\psi(0)\ra$ together evolves with time, and the resulting new state $|\psi'(t)\ra$ is just the transformed version of $|\psi(t)\ra$,
\be
\begin{gathered} \xymatrix{
|\psi(0)\ra \ar[r]^{U(t)} \ar[d]_{V} &|\psi(t)\ra \ar[d]^{V} \\
|\psi'(0)\ra \ar[r]_{U(t)} & |\psi'(t)\ra, \\
} \end{gathered}
\ee
in mathematical language,
\be
U(t)V|\psi(0)\ra = VU(t)|\psi(0)\ra \ \implies\  [V,U(t)]=0
\ee
thus it indicates that it commutes with the time-evolution operator $U$, where we use the fact that $|\psi(0)\ra$ is arbitrary. Now that we know that $U(t)=e^{-\frac i \hbar H t}$, expanding it gives us
\be
[V,H]=0.
\ee
So we conclude here that \textit{symmetry transformation is a unitary operation that commutes with the Hamiltonian, thus it is conserved.}\\
Moreover, if the symmetry transformation is continuous, we can expand it as before, $V=I-i\ep G$, where $G$ is a Hermitian operator called the \textit{generator}, which means the operation can be built up by incremental changes with it. Having the generator in hand we get a stronger result as
\be
[G,H]=0.
\ee
Thus the generator itself commutes with the Hamiltonian and is conserved. By saying that finding all the symmetries, we mean finding all the conserved quantities, as the above equation indicates.\\

As a special case, we take \textit{the space translation of the wave function} as the first example. By saying ``wave function" rather than ``states", we meant we've already choose a set of basis, so in below, an operator means it's in the basis-dependent form.

An infinitesimal space translation operator $V_x(\ep)$ takes the original wave function $\psi(x)$ to its nearest point,
\be
V_x(\ep)\psi(x)=\psi(x-\ep)=\psi-\ep\frac{\p\psi}{\p x},
\ee
so we may extract its form as
\be
V_x(\ep)=I-\ep\frac{\p}{\p x}=I-i\ep\frac{\hat p_x}{\hbar},
\ee
where $\frac{\hat p_x}{\hbar}$ serves as its generator. To judge whether $V_x$ is a symmetry transformation, we see that whether the generator $\hat p_x$ commutes with the Hamiltonian. For a free particle, the Hamiltonian is given as
\be
H=\frac{\hat p_x^2}{2m}+\frac{\hat p_y^2}{2m}+\frac{\hat p_z^2}{2m},
\ee
we see that it complies with 
\be
[\hat p_x,H]=0,
\ee
since $\hat p_x$ commutes with itself and it commutes with $\hat p_y$ and $\hat p_z$. So $V_x$ is indeed a translation symmetry under the condition of a free particle.\\

As another case, we discuss the \textit{space rotation of the wave function}. The wave function has a particular form on a circular wire, as $\psi(\theta)$. \\

\psset{linewidth=0.4pt}
\begin{pspicture}(-7,-2.5)(7,2.5)
   \psaxes[labels=none,ticks=none]{->}(0,0)(-2,-2)(2,2)
   \pscircle[linewidth=0.8pt](0,0){1.5}
   \pswedge[linewidth=0.8pt](0,0){1.5}{0}{60}
   \pswedge[linewidth=0.8pt](0,0){0.5}{0}{60}
   \uput[30](0.5;30){$\th$}
\end{pspicture}

An infinitesimal rotation operator $R_\th(\ep)$ makes the wave function moves around the circle at an infinitesimal angle $\th$ anti-clockwise.
\be
R_\th(\ep)\psi(\th)=\psi(\th-\ep)=\psi-\ep\frac{\p \psi}{\p \th},
\ee
so the infinitesimal change is 
\be
\del\psi=-\ep\frac{\p\psi}{\p\th}=-i\frac{\ep}{\hbar}\1(-i\hbar\frac{\p\psi}{\p\th}\2)\equiv-i\frac \ep \hbar L\psi,
\ee
where $L\equiv-i\hbar\frac{\p}{\p\th}$ serves as the generator of the rotation, which we will see is the angular momentum operator. Since the Hamiltonian is expressed in terms of the angular momentum as:
\be
H=\frac{L^2}{2I},
\ee
where $I$ is the rotation inertia, so the angular momentum commutes with the Hamiltonian $[L,H]=0$, thus it is a conserved quantity and $R_\th$ is a rotation symmetry.\\
Next we want to explore the eigenvalues and eigenvectors of the angular momentum operator. The eigen-equation is 
\be
-i\hbar\frac{\p}{\p\th}\psi(\th)\equiv L\psi(\th)=l\psi(\th),
\ee
where $l$ is its eigenvalue (of $z$-component of angular momentum), and the eigenfunction is solved as
\be
\psi(\th)=e^{i\frac l\hbar \th} \psi(0)\equiv e^{im \th} \psi(0),
\ee
where $m$ is multiples of $\hbar$ by $l$. Note that in this form we have an additional constraint, which says that rotating the function by the angle of $2\pi$, physical observables should stay the same, for bosons the wave function also stays the same, for fermions the wave function may differ by a phase factor of $-1$, thus corresponding to the fact that $m$ must be an integer or half-integer, which we will discuss later.

\subsection{Symmetry operations and Energy degeneracy} 

In the rotation symmetry case we discussed above, we see that the hamiltonian, or the energy, depends only on the magnitude of the angular momentum, but not on the direction of it, so in this case we have
\be
E(m)=E(-m),
\ee
which says that the energies are the same in both anti-clockwise and clockwise cases of angular momentum $m$, or the energy level $m$ is \textit{degenerate}. \textit{Degeneracy} means there are more than one state at the same energy level. Symmetry sometimes implies degeneracy, but sometimes only one symmetry is not enough to tell. 

In the above case, it seems that rotation symmetry leads to the degeneracy of energy level $m$, but the degeneracy will be broken by a magnetic field pointing outwards or inwards the plane. It'll raise the energy a little bit of the angular momentum in one direction, and lower the energy a little bit of the other direction. Note that the magnetic field however does not violate the rotation symmetry, yet with its existence the degeneracy is gone. To conclude, the wave function has the rotation symmetry, and the magnetic field also has the rotation symmetry, but the energy level is not degenerate. So the rotation symmetry itself is not enough to tell the degeneracy of energy levels, we need one more symmetry which is discrete---\textit{reflection symmetry}.

In this case the reflection is about $x$-axis, we will denote the operator as $M$ which represents the ``mirror reflection", to avoid the confusion with the rotation operator $R$. The mirror reflection typically acts on the wave function as
\be
M\psi(\th)=\psi(-\th).
\ee
Here we explore the commutator of $M$ with $L$ and $H$. Note that each wave function can be decomposed in terms of the eigenfunctions of $L$, so for simplicity, we make the operator act on the eigenfuctions of $L$.
\be\1\{\begin{split}
&[M,L] e^{im\th}=MLe^{im\th}-LMe^{im\th}=me^{-im\th}-(-me^{-im\th})=2me^{-im\th}\\
&[M,L^2] e^{im\th}=ML^2e^{im\th}-L^2Me^{im\th}=m^2e^{-im\th}-(-m)^2e^{-im\th}=0,
\end{split}\2.\ee
so $M$ commutes with the Hamiltonian, thus it's a symmetry operation; but it doesn't commute with $L$.\\
Consequently, if we consider the existence of the magnetic field, we have to add to the Hamiltonian an extra term which is proportional to the product of $B$ and $L$, 
\be
H=\frac{L^2}{2I}+\gamma BL,
\ee
in this case the reflection operation will not commute with the Hamiltonian, thus it's not a symmetry.\\
To conclude, in the case without the magnetic field, both the rotation symmetry and the mirror symmetry is guaranteed, thus we have the degeneracy; but in the case with the existence of the magnetic field, the mirror symmetry is broken, thus degeneracy is also gone. \\

The reflection operation leads to the inversion of the magnetic field, turning pointing outwards into inwards, since the magnetic field is generated by the electric current, which is the actual ingredient we want reflection.
\begin{pspicture}(0,-5)(16,5)
\psline{->}(4,0)(12,0)
\psarc[arrowscale=2]{->}(8,2){1.5}{0}{360}\psarc[arrowscale=2]{<-}(8,-2){1.5}{0}{360}
\psdots[linecolor=blue,dotstyle=*,dotsize=3pt](8,2)(7,2)(9,2)(8,1)(8,3)(8.6,2.6)(8.6,1.4)(7.4,2.6)(7.4,1.4)
\psdots[linecolor=red,dotstyle=x,dotsize=5pt](8,-2)(7,-2)(9,-2)(8,-1)(8,-3)(8.6,-2.6)(8.6,-1.4)(7.4,-2.6)(7.4,-1.4)
\rput(8,-4){\textit{Reflection of the magnetic field about the $x$-axis}}
\rput(12,0.3){$x$}
\end{pspicture}

Actually we have a more accurate statement which says,\\

\textit{Symmetry operations that don't commute with each other imply degeneracy.}\\

In mathematical language, ``symmetry operations that don't commute with each other" should be written as
\be\1\{\begin{split}
&[A,H]=0\\
&[B,H]=0\\
&[A,B]=iC,
\end{split}\2.\ee
where $A$ and $B$ are generators of the corresponding symmetry. $C$ is a new operator, with its Hermitian property guaranteed by the factor of $i$ in front of it. It's easy to prove that $C$ also commutes with the Hamiltonian,
\be
[C,H]=0,
\ee
thus it's also a symmetry. By the same procedure we create $C$, we could create more symmetry generators as,
\be
[A,C]=D, \quad  [B,D]=...
\ee
We keep doing this until we don't get something new, but rarely we might never end. Regardless of endless case, we have a collection of symmetry generators, $\{A,B,C,D,...\}$, which form a closed system called a group, with the group multiplication defined as the commutation. With this definition of multiplication, the generator group is called a \textit{Lie algebra}.\\

Next we're going to explore an important case of energy degeneracy led by non-commutating symmetry operations --- three components of the 3-dimensional rotation.

If a system has rotation invariance, all the three components $L_x, L_y, L_z$ should commute with the Hamiltonian $H$,
\be
[L_i, H]=0, \quad i=x,y,z.
\ee
But do the three components commute with each other? \\
In order to figure out this more easily, we firstly express one components $L_z$ in terms of $x-y$ coordinates. $L_z$ is the generator of a rotation of a particle in 2-dimension. The rotation is led by infinitesimal changes in $x-y$ coordinates as
\be\1\{\begin{split}
&x'=x\cos{\ep}+y\sin{\ep}=x+\ep y \\
&y'=-x\sin\ep +y\cos\ep=y-\ep x,
\end{split}\2.\ee
thus the variation of the coordinates are
\be\1\{\begin{split}
&\del x=\ep y \\
&\del y=-\ep x,
\end{split}\2.\ee
and the wave function is changed by
\be\begin{split}
\del \psi&=\frac{\p\psi}{\p x}\del x+\frac{\p\psi}{\p y}\del y\\
&=\ep y\frac{\p\psi}{\p x}-\ep x\frac{\p\psi}{\p y}\\
&=-i\frac{\ep}{\hbar}(x \hat p_y-y \hat p_x)\psi,
\end{split}\ee
and in the last subsection we denote $\del\psi$ as $-i\frac{\ep}{\hbar}L_z\psi$, so 
\be
L_z=xp_y-yp_x,
\ee
which is consistent with the classical definition
\be
(L)_z=(x\times p)_z.
\ee
By the same reasoning, we give the expression of the two other components:
\be\1\{\begin{split}
&L_x=yp_z-zp_y \\
&L_y=zp_x-xp_z.
\end{split}\2.\ee
Then we can calculate the commutator of two of the components, say, $L_x$ and $L_y$,
\be
[L_x,L_y]=[yp_z-zp_y,zp_x-xp_z]=-i\hbar yp_x+i\hbar xp_y=i\hbar L_z,
\ee
where we have used the fact that $[x_i,p_j]=i\hbar \del_{ij}$. Thus we find the commutator of two symmetry generators will give a new symmetry generator, as we stated. Actually the commutators of the three components of angular momentum can be summarized as 
\be
[L_i,L_j]=i\hbar \ep_{ijk}L_k,
\ee
where $\ep_{ijk}$ is the Levi-Civita symbol. For convenience, we sometimes just drop the $\hbar$ in below.

Next we arbitrarily pick one of the components, say $L_z$, and  work with eigenvalues and eigenvectors of it. The eigen-function of $L_z$ is given by
\be
L_z|m\ra=m|m\ra.
\ee
But now let's temporarily forget the fact that ``$m$"s are integers, and explore what possible values ``$m$"s can have. As a trick, we may convert the question in another way: suppose we already have an ``$m$", how to find other values of ``$m$"? Here Dirac suggest the clever tools of raising and lowering operators of $L_z$:
\be\1\{\begin{split}
&L_+=L_x+iL_y \\
&L_-=L_x-iL_y.
\end{split}\2.\ee
It's straightforward to compute their commutators with $L_z$,
\be\1\{\begin{split}
&[L_+,L_z]=-L_+ \\
&[L_-,L_z]=L_-.
\end{split}\2.\ee
Now let's take one of the commutators to act on an eigenstate of $L_z$,
\be
-L_+|m\ra=[L_+,L_z]|m\ra=mL_+|m\ra-L_zL_+|m\ra,
\ee
by sorting the equation we have
\be
L_z\ L_+|m\ra=(m+1)\ L_+|m\ra,
\ee
this equation implies that $L_+|m\ra$ is also an eigenstate of $L_z$, and its corresponding eigenvalue is $m+1$, so we denote this eigenstate as
\be
L_+|m\ra\equiv |m+1\ra,
\ee
and the similar procedure goes with $L_-$,
\be
L_-|m\ra\equiv |m-1\ra,
\ee
where we actually omit the normalization factor in front of the state by setting it to be $1$. From these we see that the effect of $L_\pm$ is to raise or lower the energy eigenstate by one unit, thus generating spectrum of energy from a single energy eigenstate.

Since $L_z$ is symmetric when converted upside-down, thus the spectrum has no difference with that of $-L_z$, hence the spectrum is symmetric about the zero point in $m$-axis. There are two possibilities satisfying this symmetry: ``$m$"s are all integers or half integers, which represent orbital or spin angular momentum when truncated at certain maximum and minimum values. 

\psset{linewidth=0.4pt}
\begin{pspicture}(0,-4)(16,4)
\psaxes(5,0)(5,-3.5)(5,3.5) \psaxes(11,0)(11,-3.5)(11,3.5)
\psdots[linecolor=red,dotsize=5pt](5,0)(5,1)(5,2)(5,3)(5,-1)(5,-2)(5,-3)(11,0.5)(11,-0.5)
\rput(5,3.8){$m$}\rput(11,3.8){$m$}\rput(5,-4){\textit{{\color{red}Integer eigenvalues}}}\rput(11,-4){\textit{{\color{red}Half-integer eigenvalues}}}
\end{pspicture}\\

Next we rotate the wave function by an angle of $2\pi$, when ``$m$"s are integers, we have
\be
R(2\pi)\psi(\th)=\psi(\th+2\pi)=e^{i2\pi m}\psi(\th)=\psi(\th),
\ee
the wave function does not change, and neither are the physical quantities, as we expected, thus we call the particles that has the kind of wave functions, the bosons.

when ``$m$"s are half-integers as $m=n+1/2$, where $n$ is an integer, we have
\be
R(2\pi)\psi(\th)=\psi(\th+2\pi)=e^{i2\pi m}\psi(\th)=e^{i2\pi (n+\frac 1 2)}\psi(\th)=-\psi(\th),
\ee
the wave function changes by a minus sign, but the physical quantities still do not change since they contain product of conjugates of wave functions, also as we expected. But in these cases we call the particles that has this kind of wave functions, the fermions. Note that for fermions, if we want the wave function itself to be unchanged under rotation, the minimal angle would be $4\pi$.

Experimentally if we want test this phase factor of $-1$ of fermion wave function caused by rotation by $2\pi$, we choose the spin direction to denote the direction of the wave function $\psi(x,\sig)$, and align it to the magnetic field. Then we rotate the magnetic field slowly by the angle of $2\pi$, and interfere it with the original wave function, we can see the interference pattern differs from that of two wave function not rotated at all.\\



Each state point in the spectrum axis has the same energy, thus we call these states \textit{``multiplets"}. There are $2l+1$ states of the angular momentum characterized by $l$, the maximum value of and the minus minimum value of $L_z$, and there're $2$ states of the spin characterized by $\frac 1 2$. 

As an aside, the eigenstates of $L_x$ or $L_y$ are just linear combination of the ``$|m\ra$"s, which is an obvious fact.\\

We will firstly prove that different states in multiplets, say $|m\ra$ and $|m-1\ra$, have the same energy, without showing explicitly what value the energy is, and after that we calculate the energy value of all the multiplets.\\
Since the Hamiltonian is proportional to the squared angular momentum, the eigenstate of the angular momentum is also the eigenstate of the Hamiltonian, thus it has a definite energy, we set it to be $E$,
\be
H|m\ra=E|m\ra,
\ee
and we calculate the energy of the lower level by one unit, $|m-1\ra$,
\be
H|m-1\ra=HL_-|m\ra=L_-H|m\ra=L_-E|m\ra=E|m-1\ra,
\ee
where we used the fact that $[H,L_-]=0$. Hence we find that $|m-1\ra$ has the same energy with $|m\ra$. And by the same reasoning, every state in the multiplets has the same energy $E$. This degeneracy follows from that the $L_x$ and $L_y$ symmetries don't commute.\\
Next we are going to find what exact value does $E$ have. We expand the squared angular momentum as
\be\begin{split}
L^2&= L_z^2+L_x^2+L_y^2\\
&=\underbrace{L_z^2+(L_x-iL_y)(L_x+iL_y)}_{\text{classical terms}}\underbrace{-i[L_x,L_y]}_{\text{quantum correction}}\\
&=L_z^2+L_z+L_-L_+,
\end{split}\ee
and we apply it to the state $|l\ra$, where $l$ is the maximum of $m$,
\be\begin{split}
L^2|l\ra&=L_z^2|l\ra+L_z|l\ra+L_-\underbrace{L_+|l\ra}_0\\
&=(\underbrace{l^2}_{\text{classical}}+\underbrace{l}_{\text{quantum}})\quad |l\ra=l(l+1)|l\ra,
\end{split}\ee
where we used the fact that $L_+|l\ra=0$, which means the spectrum is truncated at the highest $m$, and we see that the eigenvalue of $L^2$ is $l(l+1)$, differing from that of a classical operator by a factor of $l$.\\
Next we apply the same operator on a lowest level $|l-1\ra$,
\be
L^2|l-1\ra=L^2L_-|l\ra=L_-L^2|l\ra=l(l+1)L_-|l\ra=l(l+1)|l-1\ra,
\ee
where we used that $[L^2, L_\pm]=0$ following from $[L^2,L_i]=0$. Here we didn't use directly the form of $L^2$ as $L_z^2+L_z+L_-L_+$ because we didn't get the normalization factor of $L_+$ acting on $|l-1\ra$, or more generally $|m\ra$. From here we see that when you find a multiplet like this, going from $l$ to minus $l$, such that there's no more above it or no more below it, you find not only eigenvectors of $L_z$ with eigenvalues $m$, but you also find the eigenvectors of the squared angular momentum $L^2$ with eigenvalue $l(l+1)$ where $l$ is just the maximum value of $m$; so for each one of these little $l$s, there are $2l+1$ states and they all have the same squared angular momentum, and hence the energy, expect a factor considering units.

\newpage
\section{Non-relativistic Quantum Fields}
\subsection{Quantum mechanical harmonic oscillators and ladder operators}
The Hamiltonian of a harmonic oscillator is given by
\be\begin{split}
H&=\frac {p^2} 2 +\frac{ \om^2 x^2 }2\\
&=\frac 1 2 \bigg\{(p+i\om x)(p-i\om x)-i\om [x,p]\bigg\}\\
&=\hbar \om \1(\frac{p+i\om x}{\sqrt{2\om\hbar}}\2) \1(\frac{p-i\om x}{\sqrt{2\om\hbar}}\2) +\frac{\hbar\om} 2 \\
&\equiv \hbar\om\ a^+ a^- +\frac{\hbar\om} 2\\
&\equiv \hbar\om\ \1(N+\frac 1 2\2),
\end{split}\ee
where we set the mass to be $1$ and the ``spring constant'' to be $\om^2$, and we used the commutation relation $[x,p]=i\hbar$, and induced two new symbols $a^\pm$.\\
We see that there's constant $\frac{\hbar\om} 2$ appears in the expression, we call it the ``zero point energy'', or the ground state energy, it's there even when the harmonic oscillator is in its ground state. Classically, the ground state energy of a harmonic oscillator is zero; but quantum mechanically, the position and the momentum cannot simultaneously be zero, thus the energy being square of them got to be a little energy. It's the Heisenberg uncertainty principle which tells you that you can't have zero energy in the ground state.

Since $\frac{\hbar\om} 2$ is nothing but a constant we may add anytime and it doesn't affect the eigenvectors of the Hamiltonian, we can drop it for a while until the last and then we restore it. So we list the Hamiltonian and the ladder operator, as well as a useful tool which is the commutator of the ladder operators, as below,
\be\1\{\begin{split}
&H=\hbar\om\ a^+ a^-\equiv \hbar\om N\\
&a^\pm=\frac{p\pm i\om x}{\sqrt{2\om\hbar}}\\
&[a^-,a^+]=1,
\end{split}\2.\ee
where the $a^+$ and $a^-$ are Hermitian conjugates of each other, since $p$ and $x$ are Hermitian operators.

Next we explore the eigenvectors and eigenvalues of the Hamiltonian or the number operator $N$. The eigen-equation is given by
\be
a^+a^-|n\ra\equiv N|n\ra=n|n\ra,
\ee
where $|n\ra$ is its eigenvector and $n$ its eigenvalue. According to the commutation relation of the ladder operators, we have
\be
a^+|n\ra=a^+[a^-,a^+]|n\ra=a^+a^-a^+|n\ra-a^+a^+a^-|n\ra-=Na^+|n\ra-a^+N|n\ra=Na^+|n\ra-na^+|n\ra,
\ee
by moving the last term to the left, we get
\be
N\ a^+|n\ra=(n+1)\ a^+|n\ra,
\ee
which tells us that the raising operator $a^+$ acting on the state $|n\ra$ gives us a new eigenvector of $N$ with an eigenvalue one unit up, $n+1$. And the same reasoning goes with the lowering operator $a_-$, it takes the level one unit down, except that it cannot go infinitely down, since the Hamiltonian is always positive, there's a ground state $|0\ra$ exerted by $a^-$ gives $0$,
\be
a^-|0\ra=0.
\ee
Thus, we write the effect that $a^\pm$ act on $|n\ra$ as
\be\1\{\begin{split}
a^+|n\ra&=c_n\ |n+1\ra\\
a^-|n\ra&=d_n\ |n-1\ra,
\end{split}\2.\ee
and by using the orthonormal relation 
\be
\la n|m\ra=\del_{nm},
\ee
the normalization factor can be computed as
\be\begin{split}
c_n^2&=\la n+1|c_n^2|n+1\ra=\la n|a^-a^+|n\ra=\la n|a^+a^-|n\ra+\la n|[a^-,a^+]|n\ra=n+1\quad\implies\quad c_n=\sqrt{n+1}\\
d_n^2&=\la n-1|d_n^2|n-1\ra=\la n|a^+a^-|n\ra=\la n|n|n\ra=n\quad\implies\quad d_n=\sqrt{n},
\end{split}\ee
so we can write 
\be\1\{\begin{split}
a^+|n\ra&=\sqrt{n+1}\ |n+1\ra\\
a^-|n\ra&=\sqrt{n}\ |n-1\ra,
\end{split}\2.\ee
which are consistent with the fact that $a^+a^-|n\ra=n|n\ra$ and $a^-|0\ra=0$. Every other state can be built up by acting $a^+$s on the ground state $|0\ra$. 

Next we're going to demonstrate what explicitly the ground state is, by solving the differential equation it satisfies. We work in $x$-basis, where the ground state wave function is denoted by $\la x|0\ra \equiv \psi_0(x)$, and $x$ and $p$ operators are in their basis-dependent form. Instead of solving the Schrodinger's equation directly, we may find a clever way to solve another equation which nothing but $\la x|a^-|0\ra=0$, in $x$-basis, the equation is given by
\be
\1(-i\hbar\frac{\dif}{\dif x}-i\om x\2)\psi_0(x)=0 \ \implies\ \1(\frac{\dif}{\dif x}+\frac\om\hbar x\2)\psi_0(x)=0,
\ee
we propose an ansatz in the form of 
\be
\psi_0(x)\sim e^{f(x)},
\ee
thus the differential equation becomes
\be
f'(x)+\frac\om\hbar x=0,
\ee
so the unknown function is given by
\be
f=-\frac\om{2\hbar} x^2 +C,
\ee
and the ground state function is proportional to 
\be
\psi_0(x)\sim e^{-\frac\om{2\hbar} x^2}.
\ee
This is a bell-shaped even function with no nodes, which goes to zero when $x$ approached infinity, as we aspired. picture to be added

We check this solution by inserting it into the time-independent Schrodinger's equation,
\be
-\frac{\hbar^2}{2}\frac{\dif^2}{\dif x^2} \psi(x) +\frac 1 2 \om^2 x^2 \psi(x)= E\psi(x),
\ee
for convenience, we list the first and second order derivative of $\psi_0(x)$ as follows
\be\1\{\begin{split}
\psi_0'(x)&=-\frac\om\hbar x e^{-\frac\om{2\hbar} x^2}=-\frac\om\hbar x \psi_0(x)\\
\psi_0''(x)&=-\frac\om\hbar e^{-\frac\om{2\hbar} x^2}+\frac{\om^2}{\hbar^2} x^2 e^{-\frac\om{2\hbar} x^2}=\1(\frac{\om^2}{\hbar^2} x^2-\frac\om\hbar\2)\psi_0(x),
\end{split}\2.\ee
thus the LHS of the SEQ is given by
\be
\1(-\frac 1 2 \om^2 x^2+\frac{\hbar\om}{2}+\frac 1 2 \om^2 x^2\2)\psi_0(x)=\frac{\hbar\om}{2}\psi_0(x),
\ee
joining it with RHS, we see that
\be
E_0=\frac{\hbar\om}{2},
\ee
which is the zero point energy, as we desired.

Excitation states' wave functions can be obtained by acting the raising operator onto the ground state function, with each result the same order the coefficient in front of the $\psi_0(0)$ has as the excitation level. For example, the first excitation state can be gained by
\be
\psi_1(x)=a^+\psi_0(x)=\1(-i\hbar\frac{\dif}{\dif x}+i\om x\2)\psi_0(x),
\ee
but we already knew
\be
\1(-i\hbar\frac{\dif}{\dif x}-i\om x\2)\psi_0(x)=0,
\ee
thus the first excitation wave function is
\be
\psi_1(x)=2i\om x\psi_0(x)=2i\om xe^{-\frac\om{2\hbar} x^2},
\ee
without considering the proper factor in front of it. This is an odd function with one nodes, which also shrinks to zero at infinity. picture to be added

The following second excitation wave function is straightforward, 
\be
\psi_2(0)=(a^+)^2\psi_0(x)=a^+\psi_1(x)=\1(-i\hbar\frac{\dif}{\dif x}+i\om x\2)(2i\om x\psi_0(x))=(2\om x^2-\hbar)\psi_0(x),
\ee
which becomes even again and has two nodes. picture to be added

As more and more raising operator acting on the ground state function, the resulting function becomes more and more wiggled. With the most wiggled part concentrated at the origin which represents the high momentum, and the larger amplitude away from the origin represents the big potential energy, and finally damps down to zero at infinity. This picture resembles much as the classical picture of a harmonic oscillator vibrating back and forth in a given potential.  picture to be added

\subsection{Many-harmonic-oscillator system and quantum field operators}
Consider a system containing many harmonic oscillators, each at its energy eigenstate, this state can be represented as
\be
|n_1, n_2,...,n_i,...\ra,
\ee
if we want to raise or lower the energy of the $i$th harmonic oscillator, we may make use of the ladder operator as
\be\begin{split}
a_i^+|n_1, n_2,...,n_i,...\ra&=\sqrt{n_i+1}|n_1, n_2,...,n_i+1,...\ra\\
a_i^-|n_1, n_2,...,n_i,...\ra&=\phantom{+}\sqrt{n_i}\phantom{10}|n_1, n_2,...,n_i-1,...\ra.
\end{split}\ee
Next we're going to generalize this set of notations to the many-particle system in an arbitrary potential $V(x)$. The energy eigenstate wave functions by solving the Schrodinger's equation are given by $\psi_i(x)$ (schematic picture to be added), their orthogonality relations are given by
\be\1\{\begin{split}
\int\dif x\  \psi_i^*(x)\psi_j(x)=\int\dif x\ \la i|x\ra \la x|j\ra=\la i|j\ra=\del_{ij}\\
\sum_i \psi_i^*(x)\psi_i(y)=\sum_i \la y|i\ra\la i|x\ra =\la y|x\ra=\del(x-y).
\end{split}\2.\ee
Suppose we have $n_i$ bosonic particles in the $i$th state, the state of the system is denoted by the same notation as the above many-SHO system as,
\be
|n_1, n_2,...,n_i,...\ra,
\ee
and we replace the raising/lowering operator to a set of new operators called the creation/annihilation operator,
\be\1\{\begin{split}
a^+_i &\quad\rightarrow\quad a_i^\dag\\
a^-_i &\quad\rightarrow\quad a_i,
\end{split}\2.\ee
meaning that when it acts on a state, it creates/annihilates a particle at the corresponding energy eigenstate,
\be\begin{split}
a_i^\dag|n_1, n_2,...,n_i,...\ra&:=\sqrt{n_i+1}|n_1, n_2,...,n_i+1,...\ra\\
a_i|n_1, n_2,...,n_i,...\ra&:=\phantom{+}\sqrt{n_i}\phantom{10}|n_1, n_2,...,n_i-1,...\ra,
\end{split}\ee
where the normalization factor is a convention satisfying
\be\begin{split}
a_i^\dag a_i|n_1, n_2,...,n_i,...\ra&=n_i|n_1, n_2,...,n_i+1,...\ra\\
a_i|n_1, n_2,...,0\ ,...\ra&=0,
\end{split}\ee
where we denote $a_i^\dag a_i\equiv N_i$ as the occupation operator, and the commutator is easily computed as
\be
[a_i,a_j^\dag]=\del_{ij},
\ee
the similar          result as before.\\
Specially, when the creation/annihilation operator acts on a single particle state, it gives 
\be\begin{split}
a_i^\dag|0\ra&=|i\ra\\
a_i|j\ra&=\del_{ij}|0\ra,
\end{split}\ee
where $|i\ra$ is the $i$th energy eigenstate, and in $x$-basis,
\be
\la x|i\ra=\psi_i(x).
\ee
Next, importantly, we introduce two field operators defined as 
\be\1\{\begin{split}
\hat\Psi^\dag(x)&:=\sum_i a_i^\dag\psi_i^*(x) \\
\hat\Psi(x)&:=\sum_i  a_i\psi_i(x).
\end{split}\2.\ee
Note that these``field operators" are actually ``operator fields", meaning that at each point $x$ in the space there's an operator $\Psi^\dag$ and $\Psi$. Then we are going to see what effects these operators have, firstly by acting the $\hat\Psi^\dag(x)$ on a vacuum state,
\be
\hat\Psi^\dag(x)|0\ra=\sum_i  \psi_i^*(x)a_i^\dag|0\ra=\sum_i |i\ra\la i|x\ra=|x\ra,
\ee
it means that we created a particle at point $x$! By the similar reasoning, $\hat\Psi(x)$ will annihilate a particle at point $x$,
\be
\hat\Psi(x)|y\ra=\sum_i \psi_i(x)a_i \sum_j|j\ra\la j|y\ra=\sum_{ij}  \psi_i(x) \del_{ij}|0\ra\psi_j^*(y) =\sum_i  \psi_i(x) \psi_i^*(y)|0\ra=\del(x-y)|0\ra,
\ee
which means if this particle is located at $x$, the operator annihilates it and gives a vacuum state (though with a divergent factor), and if this particle is at another position $y$, the operator just kills it to give $0$. Instead of creating/annihilating a particle at definite energy as $a_i^\dag$ and $a_i$, $\Psi^\dag(x)$ and $\Psi(x)$ creates/annihilates a particle at definite position.

The only non-vanishing commutation relation is computed as
\be\begin{split}
[\Psi(x),\Psi^\dag(y)]&=\1[\sum_i a_i\psi_i(x),\sum_j a_j^\dag\psi_i^*(y)\2]\\
&=\sum_{ij}\psi_i(x)\psi_i^*(y)[a_i,a_j^\dag]\\
&=\sum_{ij}\psi_i(x)\psi_j^*(y)\del_{ij}\\
&=\sum_{i}\psi_i(x)\psi_i^*(y)=\del(y-x).
\end{split}\ee
Next we are going to look at an interesting quantity composed of the creation and annihilation operators,
\be
\hat N\equiv \int \dif x\ \Psi^\dag(x)\Psi(x)=\int \dif x \sum_i \psi_i^*(x)a_i^\dag\sum_j  \psi_j(x)a_j=\sum_{ij}\del_{ij}a_i^\dag a_j=\sum_i a_i^\dag a_i=\sum_i N_i,
\ee
where we get nothing but the total number of particle at all energy levels, or in all field; thus $\Psi(x)^\dag\Psi(x)$ represents the particle number density at the given point $x$. This is to be compared with the equation that
\be
1=\int \dif x\ \psi^*(x)\psi(x),
\ee
where the $\psi(x)$ is the normal quantum wave function; the $1$ here represents the total probability, or incidentally also represents the particle number. The similar form by substituting the wave function to the field operator is inspiring in the next few cases.

In quantum mechanics, the average value of a observable is given by the integral of the corresponding Hermitian operator sandwiched by the wave function and its complex conjugate, for example, the average energy is 
\be
\la H\ra=\int \dif x\ \psi^*(x) H\psi(x);
\ee
in quantum field theory, the corresponding operator has the similar structure except that replacing the wave function by the field operator, e.g,
\be\begin{split}
\hat H_{field}&=\int\dif x\ \Psi^\dag(x) \hat H_{par}\Psi(x)\\
&=\int\dif x\ \sum_i \psi_i^*(x)a^\dag_i \hat H_{par}\sum_j \psi_j(x) a_j\\
&=\int\dif x\ \sum_{ij} \psi_i^*(x) a_i^\dag \ep_j \psi_j(x) a_j\\
&=\sum_{ij} \del_{ij}a_i^\dag a_j \ep_j=\sum_i a_i^\dag a_i \ep_i=\sum_i \hat N_i\ep_i
\end{split}\ee
where we have used the time-independent Schrodinger's equation, i.e, the eigen-equation of energy $\hat H_{par}\psi_i(x)=\ep_i\psi_i(x)$. The result is as we expected. And the same goes with the field momentum operator
\be\begin{split}
\hat P_{field}&=\int \dif x\ \Psi^\dag(x) \hat P_{par}\Psi(x)= -i\hbar \int \dif x\ \Psi^\dag(x)\frac{\p}{\p x}\Psi(x)\\
&=\int\dif x\ \sum_i \psi_i^*(x)a^\dag_i \hat P_{par}\sum_j \psi_j(x) a_j\\
&=\int\dif x\ \sum_i \psi_i^*(x)a^\dag_i p_j\sum_j \psi_j(x) a_j\\
&=\sum_i a_i^\dag a_i p_i=\sum_i N_i p_i
\end{split}\ee
Next we're going to develop the field operator of free particles, i.e, without any potential energy $V(x)$. In this case, the Hamiltonian is written as $H=p^2/{2m}$, the momentum operator commutes with the Hamiltonian, thus the momentum eigenfunctions are the same with the energy eigenfunctions. The momentum eigenfunction in $x$-basis is nothing but the plane wave $e^{ipx}/\sqrt{2\pi}$, thus the sum in the definition of the creation/annihilation operator becomes an integral,
\be\1\{\begin{split}
\Psi^\dag(x)&=\frac 1{\sqrt{2\pi}}\int\dif p\ a^\dag(p)\ e^{-ipx} \\
\Psi(x)&=\frac 1{\sqrt{2\pi}}\int\dif p\ a(p)\ e^{ipx},
\end{split}\2.\ee
with $a^\dag_p$ and $a_p$ meaning creating/annihilating a particle at the corresponding momentum. The creation/annihilation operator in position and momentum basis are just Fourier transformations of each other, so the inverse relation is given by
\be\1\{\begin{split}
a^\dag(p)&=\frac 1{\sqrt{2\pi}}\int\dif x\ \Psi^\dag(p)\ e^{ipx} \\
a(p)&=\frac 1{\sqrt{2\pi}}\int\dif x\ \Psi(p)\ e^{-ipx},
\end{split}\2.\ee\\

To proceed, let's study the Hamiltonian of a free particle field, which is given by
\be
H=mc^2\int\dif x\ \Psi^\dag(x)\Psi(x)+\int\dif x\ \Psi^\dag(x)  \1(-\frac{\nabla^2}{2m}\2)\Psi(x),
\ee
where the first term is the rest mass energy of the field, and the second term is the kinetic energy of the field. 

Recall that the Hamiltonian governs the time-evolution of the state, or to say, to update the state; so if in this updating process a certain quantity is unchanged, we say that this quantity is conserved. Let's study the first term, which is proportional to the total particle number in the field,
\be\begin{split}
\int\dif x\ \Psi^\dag(x)\Psi(x)&=\frac 1 {2\pi}\int\dif x\int\dif p\ a^\dag(p) \ e^{-ipx} \int\dif q\ a(q)\ e^{iqx}\\
&=\int\dif p\ a^\dag(p)\int\dif q\ a(q)\ \1(\frac 1 {2\pi}\int\dif x\ e^{i(q-p)x}\2)\\
&=\int\dif p\ a^\dag(p)\int\dif q\ a(q)\ \del(q-p)\\
&=\int\dif p\ a^\dag(p)\ a(p).
\end{split}\ee
When this term acts on a state, it finds a particle at momentum $p$ and annihilates it and then creates a particle at the same momentum, in this way, the momentum is conserved. The working ingredient here is the integral of the exponential gives the Dirac-delta function
\be
\frac 1 {2\pi} \int\dif x\ e^{i(q-p)x}=\del(q-p),
\ee
so we can say that the delta function is the delta function of momentum conservation.

Then we can generate this type of terms to any term has the following form:
\be
\int\dif x\ \Psi_A^\dag(x)\Psi_B^\dag(x)\Psi_A(x)\Psi_B(x),
\ee
where different letters denote different type of particles. And this term means that two particles $A$ and $B$ disappear and they reappear, i.e, they scatter off each other. The integral over space guarantees that wherever the scattering happens, this term always takes it into account. No matter how many particle type creation/annihilation operators there in a term, the integral will always contain the working ingredient term
\be
\frac 1 {2\pi} \int\dif x\ e^{i(q_A+q_B-p_A-p_B)x}=\del(q_A+q_B-p_A-p_B),
\ee
the total momentum will always be conserved.

As another example, let's consider
\be
\int\dif x\ \Psi_A^\dag(x)\Psi_B^\dag(x)\Psi_C(x),
\ee
which describes a decaying process --- a particle $C$ becomes particles $A$ and $B$, and the same momentum conservation ingredient is still the same. But note that this term is not Hermitian so if it's in the Hamiltonian we must add its Hermitian conjugate which represents the inverse process --- particles $B$ and $C$ becomes $A$. And usually a coupling constant denoted by $g$ reflecting the strength of the process, mostly measured in the experiments, is given in front of the term. So we may rewrite the term as
\be
g\int\dif x\ \1(\Psi_A^\dag(x)\Psi_B^\dag(x)\Psi_C(x)+\Psi_C^\dag(x)\Psi_B(x)\Psi_A(x)\2).
\ee

If we further expand the evolution operator to the second order, we will have the $H^2$ term. Considering the decay process in the above, we will find terms as
\be\1\{\begin{split}
&g\int\dif x\ \Psi_A^\dag(x)\Psi_B^\dag(x)\Psi_C(x)\Psi_C^\dag(x)\Psi_B(x)\Psi_A(x)\\
&g\int\dif x\ \Psi_C^\dag(x)\Psi_B(x)\Psi_A(x)\Psi_A^\dag(x)\Psi_B^\dag(x)\Psi_C(x),
\end{split}\2.\ee
where the first term represents $A$ and $B$ combined to form $C$ then $C$ decays into $A$ and $B$ again, just like a scattering process of $A$ and $B$, except for that it doesn't happen at a point, but with $C$ as the intermediate particle; the second term represents $C$ becomes $C$ again, with $A$ and $B$ being the temporary particles.\\

Then we study the second term of the free particle field Hamiltonian,
\be\begin{split}
H&=\int\dif x\ \Psi^\dag(x)  \1(-\frac{\nabla^2}{2m}\2)\Psi(x)\\
&=\frac 1 {2\pi}\int\dif x\ \int\dif p\   a^\dag(p) \ e^{-ipx} \1(-\frac{1}{2m}\frac{\p^2}{\p x^2}\2)\int\dif q\ a(q)\ e^{iqx}\\
&=\frac 1 {2\pi}\int\dif x\ \int\dif p\   a^\dag(p) \ e^{-ipx} \1(\frac{q^2}{2m}\2)\int\dif q\ a(q)\ e^{iqx}\\
&=\frac{1}{2m}\int\dif p\ p^2a^\dag(p)\ a(p),
\end{split}\ee
which also preserves the conservation of momentum.

\subsection{Exchange properties for bosons and fermions}
Previously we talked a little bit about bosons and fermions in terms of the phase factor caused by rotating their single particle wave function. Now we come to the second property of these two species of particles, or to say, another equivalent definition of them.

For identical particles, if we exchange the positions, or to be more precise the states of them, the physical properties of the system should remain the same, which is intuitive since they are ``identical''. But the wave function describing the system may differ by a phase factor since this does not affect the physical observables. According to the different phase factors, particles can be divided into two kinds --- fermions and bosons. In the following, we will discuss the classification both in quantum mechanical sense and quantum field sense.\\

Quantum mechanically, a state of a system consists of two particle at position $x$ and $y$ is denoted as $|x,y\ra$, naturally the corresponding wave function is given by $\psi(x,y)=\la x,y|\psi\ra$. We denote the ``swopping operator" as $S$, when it acts on the state of the system, it just switches the positions of the two particles,
\be S|x,y\ra=|y,x\ra, \ee
and when acting twice the operator on the state, it must turn back into the original state, thus the square of the swopping operator must be the unit operator,
\be S^2|x,y\ra=S|y,x\ra=|x,y\ra \quad\implies\quad S^2=I, \ee
which mathematically says that the eigenvalues of $S$ would be $\pm 1$.
The identical particles' state described by the eigenvector corresponding to the eigenvalue $1$ are called bosons; and particles are called fermions if their state corresponds to the eigenvalue of $-1$,
\be\begin{split}
S|x,y\ra&=|y,x\ra=\phantom{-}|x,y\ra, \quad\quad\text{Bosons};\\
S|x,y\ra&=|y,x\ra=-|x,y\ra, \quad\quad\text{Fermions}.
\end{split}\ee
Pauli exclusion principle goes naturally out of this definition for fermions, if two particles are put in the same state (here the same positon), the equation goes like \be |x,x\ra=-|x,x\ra\implies|x,x\ra=0,\ee which says there is no such a state existed, or, you can't put two particles in the same position. Naturally these can also be represented by wave functions, which we won't bother writing and will directly use it below.

If we refer the state of a particle as the energy level, without considering the spin degree of freedom, let's see how we construct boson and fermion systems and how Pauli exclusion principle works. For convenience, we will work in wave function representation here. If two particles are independent of each other, naming they don't have interactions, the wave function of the two-particle system can be written as the product of the single particle wave function of the two particles, \be \psi(x,y)=\psi_1(x)\psi_2(y), \ee with $1$ and $2$ representing two energy levels, same or different. If the two particles are in the same energy level, say $0$, the wave function is then symmetric under exchanging the two particles,
\be S\psi(x,y)=\psi(y,x)=\psi_0(y)\psi_0(x)=\psi_0(x)\psi_0(y)=\psi(x,y), \ee
in this sense the particles must be bosons. If the two particles are in different energy levels, swopping the two particles doesn't give the minus or the original wave function at all,
\be S\psi(x,y)=\psi(y,x)=\psi_1(y)\psi_2(x) \ne \pm\psi_1(x)\psi_2(y)=\pm \psi(x,y). \ee
But we may construct new wave functions by superposition of the original and the swopped ones, with plus or minus operation giving symmetric or anti-symmetric wave function,
\be\1\{\begin{split}
\psi_S(x,y):=\psi(x,y)+S\psi(x,y)&=\psi_1(x)\psi_2(y)+\psi_1(y)\psi_2(x)\\
\psi_A(x,y):=\psi(x,y)-S\psi(x,y)&=\psi_1(x)\psi_2(y)-\psi_1(y)\psi_2(x),
\end{split}\2.\ee
and it is easy to verify their symmetric/anti-symmetric property by acting the swopping operator on,
\be\1\{\begin{split}
S\psi_S(x,y)=S\psi(x,y)+S^2\psi(x,y)&=S\psi(x,y)+\psi(x,y)=\phantom{-}\psi_S(x,y)\\
S\psi_A(x,y)=S\psi(x,y)-S^2\psi(x,y)&=S\psi(x,y)-\psi(x,y)=-\psi_A(x,y).
\end{split}\2.\ee
Thus the symmetric wave function describes a system of two bosons, and the anti-symmetric wave function describes a system of two fermions. For the boson system, if we put two particles in the same position or energy level, the wave function just doubles; but for the fermions system, if we do the same thing, the wave function just goes to zero,
\be\1\{\begin{split}
\psi'_S(x,y)&=\psi_1(x)\psi_1(y)+\psi_1(y)\psi_1(x)=2\psi_1(x)\psi_1(y)\\
\psi_S(x,x)&=\psi_1(x)\psi_2(x)+\psi_1(x)\psi_2(x)=2\psi_1(x)\psi_2(x)\\
\psi'_A(x,y)&=\psi_1(x)\psi_1(y)-\psi_1(y)\psi_1(x)=0\\
\psi_A(x,x)&=\psi_1(x)\psi_2(x)-\psi_1(x)\psi_2(x)=0,
\end{split}\2.\ee
with the primed wave function denoting putting particles in the same energy level and $\psi(x,x)$ denoting putting particle in the same position. The zero wave function for fermions says that the probability for finding such a state is $0$, or to say, this kind of situation where two particles are in the same state cannot happen at all. This is also a natural manifestation of the Pauli exclusion principle, and this kind of symmetric and anti-symmetric wave functions occurs frequently in particle physics multiplets, thus one should get familiar with them.

The above logic goes the same with the system having more than two particles, denoted as the multiple-particle state $|x_1,x_2,x_3,...\ra$ and system with states denoted by both position and spin $|x_1,\sig_1;x_2,\sig_2;x_3,\sig_3;...\ra$.\\

In quantum field theory, the state of a system of two particles is generated by acting two creation operators on the vacuum state, $|x,y\ra=\Psi^\dag(x)\Psi^\dag(y)|0\ra$. From the swopping properties, we can obtain the commutation relations for boson fields and the anti-commutation relations for fermion fields,
\be\begin{split}
\Psi_B^\dag(x)\Psi_B^\dag(y)|0\ra&=\phantom{-}\Psi_B^\dag(y)\Psi_B^\dag(x)|0\ra \quad\implies\quad \1[\Psi_B^\dag(x),\Psi_B^\dag(y)\2]=0 \quad\quad\text{\ Bosons};\\
\Psi_F^\dag(x)\Psi_F^\dag(y)|0\ra&=-\Psi_F^\dag(y)\Psi_F^\dag(x)|0\ra \quad\implies\quad \1\{\Psi_F^\dag(x),\Psi_F^\dag(y)\2\}=0\quad\quad\text{Fermions}.
\end{split}\ee
And still, mathematical consistency of the Pauli exclusion principle goes well with creation operators' representation,
\be
\1\{\Psi_F^\dag(x),\Psi_F^\dag(x)\2\}=2\Psi_F^\dag(x)\Psi_F^\dag(x)=0 \quad\implies\quad \Psi_F^\dag(x)\Psi_F^\dag(x)|0\ra=0,
\ee
meaning that you cannot create two particles at the same point from the vacuum.

Now that the relation of fermion field operators are given by anti-commutators, we go further to derive the anti-commutator for annihilation and creation operators, by acting it on the vacuum state
\be\begin{split}
\1\{\Psi_F(x),\Psi_F^\dag(y)\2\}|0\ra&=\Psi_F(x)\Psi_F^\dag(y)|0\ra+\cancel{\Psi_F^\dag(y)\Psi_F(x)|0\ra}=\Psi_F(x)|y\ra=\del(x-y)|0\ra,
\end{split}\ee
and on single particle state $|y\ra$ as
\be
\1\{\Psi_F(x),\Psi_F^\dag(y)\2\}|y\ra=\cancel{\Psi_F(x)\Psi_F^\dag(y)|y\ra}+\Psi_F^\dag(y)\Psi_F(x)|y\ra=\del(x-y)\Psi_F^\dag(y)|0\ra=\del(x-y)|y\ra,
\ee
thus we may conclude 
\be
\1\{\Psi_F(x),\Psi_F^\dag(y)\2\}=\del(x-y).
\ee
Yet this anti-commutator acting on $|x\ra$ will result in some subtitles which are not important, so we just ignore it.

The creation/annihilation operators of fermions of a certain energy level also takes the anti-commutation relations, which are given as
\be
\{a_F,a_F^\dag\}=1 \Longleftrightarrow \{a_F^\dag, a_F\}=1,
\ee
this is easy to be proved by acting this to the no-particle state or 1-particle state on that given energy level,
\be\begin{split}
\{a_F,a_F^\dag\}|0\ra&=a_Fa_F^\dag |0\ra+\cancel{a_F^\dag a_F|0\ra}=a_F|1\ra=|0\ra\\
\{a_F,a_F^\dag\}|1\ra&=\cancel{a_Fa_F^\dag |1\ra}+a^\dag a_F|1\ra=a_F^\dag|0\ra=|1\ra.
\end{split}\ee
Observing the anti-commutation relations, we see that from the algebraic structure, there's no clear distinction between the creation and annihilation operator; they are totally symmetric in mathematical sense. On contrast, the commutation relations of boson creation and annihilation operators show a clear unequal footing of them,
\be [a_B,a_B^\dag]=1 \Longleftrightarrow [a_B^\dag,a_B]=-1. \ee
This is basically due to the physical fact of the following. For fermions the 0-particle state and the 1-particle state are the only two possible state on a certain energy level, thus they are treated as equal --- acting the creation operator twice on the 0-particle state will give zero, as well as acting the annihilation operator twice on the 1-particle state, the particle number axis can turn upside-down without any difference to be seen. But for bosons the 0-particle state is the ground state, above it, there are 1-particle, 2-particle, ... states etc, hence there is no reason we can treat them equal, to illustrate more explicitly, for a state which number of particles is given, one can always act the creation operators on it and never get zero, but acting the annihilation operators on for finite number of times, one can finally get zero; you turn upside-down the particle number axis, it's not as same as before, i.e, it's asymmetric. (Figures of boson and fermion particle number axis and the effect of creation/annihilation operators are to be added.)

One last thing, we recover the anti-commutation relation of the field operators from the generalized anti-commutation relation of the annihilation/creation operators of two energy levels $i$ and $j$,
\be \{a_i,a_j^\dag\}=\del_{ij}. \ee
The approach is by simply using the definition of the field operators,
\be\begin{split}
\1\{\Psi_F(x),\Psi_F^\dag(y)\2\}&=\1\{\sum_i a_i \psi_i(x),\sum_j a_j^\dag \psi_j^*(y)\2\}\\
&=\sum_{ij}\psi_i(x)\psi_j^*(y)\1\{a_i,a_j^\dag\2\}=\sum_{ij}\psi_i(x)\psi_j^*(y)\del_{ij}=\sum_i\psi_i(x)\psi_i^*(y)=\del(x-y),
\end{split}\ee
which is consistent with the result when we act the field anti-commutator on single-particle states $|0\ra$ and $|y\ra$.


\section{Relativistic Quantum Mechanics and Quantum Field Theory}
\subsection{Dirac equation in one-dimension and the negative energy problem}
When dealing the case of particle moving very fast, comparable with the speed of light, we need a relativistic version of the quantum mechanics. In special relativity, the relation between the energy, momentum and the mass is given by
\be\label{ener}
E^2=p^2+m^2
\ee
When we replace the quantity to the corresponding operators as
\be\1\{\begin{split}
E &\quad\rightarrow \quad i\frac{\p}{\p t}\\
p &\quad\rightarrow -i\frac{\p}{\p x},
\end{split}\2.\ee
we have 
\be
\frac{\p^2\psi}{\p t^2}-\frac{\p^2\psi}{\p x^2}=m^2\psi,
\ee
which is the Klein-Gorden equation. But instead of a second order differential equation, we want a first order one, which is relevantly more difficult.\\

To begin with, we consider a massless particle moving with the speed of light in one dimension, the energy relations reads as
\be
E=\pm p,
\ee
which can represent right moving particles with $p$ itself positive, or left moving particles with $p$ itself negative; thus no matter which direction the particle is moving, the energy can either be positive or negative (noting the two $\pm$ signs). We stick with right-moving particles, and with the substitution we have
\be
i\frac{\p\psi}{\p t}=\mp i\frac{\p\psi}{\p x} \quad\text{or}\quad \frac{\p\psi}{\p t}\pm\frac{\p\psi}{\p x}=0, explanation\ in\ below\ should\ be\ modified!!
\ee
the solution has the form of $\psi(x\mp t)$ describes a right/left moving wave function with the speed of light $1$. Dirac treat the right/left moving property as a degree of freedom, thus any wave function has two components representing right/left moving just as the spin. In this treatment, the above equations can be composed into a single one in matrix version
\be
i\frac{\p}{\p t}\binom{\psi_1}{\psi_2}=-i\1(\ba{cc}1&0\\0&-1\ea\2)\frac{\p}{\p x}\binom{\psi_1}{\psi_2},
\ee
in this way, the Hamiltonian is given in terms of $p$ as
\be
H=\al p,
\ee
where $\al$ is the third Pauli matrix. When it acts on the right-moving state, it gives the eigenvalue of $1$; when it acts on the left-moving state, it gives the eigenvalue of $-1$,
\be\1\{\begin{split}
\1(\ba{cc}1&0\\0&-1\ea\2)\binom{\psi_1}{0}&=\phantom{-}\binom{\psi_1}{0}\\
\1(\ba{cc}1&0\\0&-1\ea\2)\binom{0}{\psi_2}&=-\binom{0}{\psi_2}.
\end{split}\2.\ee
Then we add a mass term to this Hamiltonian,
\be
H=\al p+\bet m,
\ee
the new symbol $\bet$ denoting also a matrix due to the consistency of the matrix form. If we square it, we should recover the energy relation of the massive particle eq \eqref{ener}, which requires
\be\1\{\begin{split}
&\al^2=1\\
&\bet^2=1\\
&\al\bet+\bet\al=0,
\end{split}\2.\ee
and we know the $\bet$ matrix which satisfies this can be any of others Pauli matrices. We set it to be 
\be 
\bet=\1(\ba{cc}0&1\\1&0\ea\2).
\ee
Now we apply this new first order Hamiltonian to a two-component state to get
\be
i\frac{\p}{\p t}\psi=-i\al\frac{\p}{\p x}\psi+\bet m\psi,
\ee
or explicitly in its matrix form
\be
i\frac{\p}{\p t}\binom{\psi_1}{\psi_2}=-i\1(\ba{cc}1&0\\0&-1\ea\2)\frac{\p}{\p x}\binom{\psi_1}{\psi_2}+m\1(\ba{cc}0&1\\1&0\ea\2)\binom{\psi_1}{\psi_2},
\ee
which can be separately written as 
\be\1\{\begin{split}
i\frac{\p\psi_1}{\p t}+i\frac{\p\psi_1}{\p x}&=m\psi_2\\
i\frac{\p\psi_2}{\p t}+i\frac{\p\psi_2}{\p x}&=m\psi_1.
\end{split}\2.\ee
We see that when we introduce the mass term, the two components of the state couple with each other, they're not independent any more. Or to say inversely, the coupling of the two components of the state gives the mass of the particle, which makes the particle move slower than the light. This is consistent with physical significance of a massive particle --- the right moving particle can be transformed into the left moving particle in a reference frame which moves faster than the particle, while the massless particle moving in the speed of light doesn't have this property since no reference frame can move faster than the light. 

Next we come to the problem of the negative energy.\\
The vacuum is the state that all the negative energy states are filled with fermions like electrons, which are called the Dirac sea. When a photon hits the vacuum, the energy excites an electron at a negative energy level to a positive energy level, thus leaving a hole in the negative energy sea. The effect of the hole is in general to have a positive charge in the sea, which we call a positron. This is the mechanism of how a photon creates a electron/positron pair. (figure to be added)\\
The Dirac field operator is constructed as
\be\begin{split}
\Psi(x)&=\int_{-\infty}^{\infty}\dif p\ a(p) e^{ipx}\\
&=\int_{0}^{\infty}\dif p\ a(p) e^{ipx}+\int_{-\infty}^{0}\dif p\ a(p) e^{ipx}\\
&=\int_{0}^{\infty}\dif p\ a(p) e^{ipx}+\int^{\infty}_{0}\dif p'\ b^\dag(p') e^{-ip'x},
\end{split}\ee
where annihilation operators for electrons can be seen as creation operators for positions. Since the annihilation and creation operators for fermions are on the same footing, the creation operators would take the same form.

In this sense, a term like $\Psi^\dag\Psi A$, where $A$ represents the photon field operator, contains four separate terms. Thus the original single term could contain multiple processes as: an electron emits a photon, an electron and a positron annihilate and create a photon, the vacuum spontaneously create an electron and a positron and a photon, etc. (Figures of Feynman diagrams of the three processes to be added.)

\subsection{Dirac equation in three-dimension and chirality}
Now we generalize our relativistic equation into 3-dimensions. Still, we study massless particles and then add the mass term. In 3D case, the momentum must have three components, thus it's a vector; but the energy remains a scalar which has rotation invariance. This requires the $\al$ matrix also has three component, so as to get a ``dot product" with the momentum to get an invariant scalar, in this sense $\al$ can also be treated as a ``vector". Therefore the Hamiltonian can be generalized to
\be H=\vec\al\cdot\vec p=\al_x p_x+\al_y p_y+\al_z p_z, \ee
and we want this generalization to satisfy the good old energy-momentum relation,
\be H^2=\vec p^2=p_x^2+p_y^2+p_z^2, \ee
which requires the $\al$ matrices to have the property as
\be\1\{\begin{split}
\al_i^2&=1\\
\{\al_i,\al_j\}&=0 \quad\text{for}\quad i\ne j,
\end{split}\2.\ee
or to be summarized as
\be \{\al_i,\al_j\}=2\del_{ij}. \ee
There are three matrices which can satisfy all the above conditions, Pauli matrices; thus we naturally attribute $\al$ matrices as $\al_i=\sig_i$, hence the Hamiltonian is given by \be H=\vec\sig\cdot\vec p. \ee Since the Pauli matrices are attached to the spin, this equation would serve as the origin of the spin. we don't see spin in 1-dimension, we only see spin-like things as the left/right moving property; spin is truly seen in 3-dimension. Additionally, in this form the energy is proportional to the component of the spin along the direction of motion. But we know the component of the spin is always $\pm 1$, so in one case the spin is pointing along the momentum giving positive energy, and in the other case the spin is opposite to the momentum giving the negative energy. Again, we restate that, for a massless particle with positive energy, the spin is always locked to the direction of the momentum.

And then we follow the old steps to add the mass term multiplied by a matrix which has the same dimension with that of $\al_i$,
\be H=\vec\sig\cdot\vec p+\bet m, \ee
and make it to satisfy
\be H^2=p^2+m^2, \ee
which additionally requires the $\bet$ matrix to satisfy
\be\1\{\begin{split}
\bet^2&=1\\
\{\bet,\al_i\}&=0.
\end{split}\2.\ee
But unfortunately, there's not a forth $2\times 2$ matrix beyond the three Pauli matrices which could both squares to be $1$ and anti-commutes with the $\al_i$s. Nor can we find four $3\times 3$ matrices which can simultaneously meet all the requirements. The minimal dimension we can find such four matrices will be $4\times 4$. Incidentally, there are many representations of Dirac matrices, we just have to pick one and stick with it. Thus the wave function would have two more degrees of freedom, in total four components, together known as a \textbf{spinor}. But note that these four components have nothing to do with the $3+1$ dimensional spacetime, their existence is just a mathematical result. In 4D spinor space, we rewrite the Hamiltonian once more as 
\be H=\vec\al\cdot\vec p+\bet m, \ee
and one set of $4\times 4$ matrices called the Pauli representation of the Dirac matrices are given as
\be 
\vec\al=\1(\ba{cc}\vec\sig&O\\O&-\vec\sig\ea\2) \quad\quad \bet=\1(\ba{cc}O&I\\I&O\ea\2),
\ee
where $O$, $I$ and $\vec\sig$ are themselves $2\times 2$ matrices. One can check them by squaring the Hamiltonian to give the energy-momentum relation.

A property describes the relations between the direction of the spin and the momentum is called the \textbf{chirality}. A particle with the spin and the momentum in the same direction is \textit{right-handed}, a right-handed particle has both positive energy and negative energy components; a particle with the spin and the momentum in the opposite direction is \textit{left-handed}, a left-handed particle has also both positive energy and negative energy components. To leave the negative energy away for the moment, we restate that for positive energy particles, they can be classified into right-handed and left-handed ones. When the two chiral components are coupled together by the $\bet$ matrix, the mass term of the particle comes out; or to say, the effect of the mass of a Dirac particle is to make transition of the two chiral particles; this is consistent with the fact that one can choose a reference frame which moves faster than the massive particle and sees the momentum pointing in the opposite direction (with spin direction unchanged), thus from his point of view the right-handedness becomes left-handedness. In this sense, we conclude that for massive particles, chirality depends on the reference frame. On contrast, for a massless particle the chirality is conserved, it stays right-handed or left-handed forever, which is also consistent with the fact that nothing can move faster than light and change the direction of  the momentum.

Next we're going to leave the dot product away and study the $\vec\al$ matrices alone. It's physical meaning, or to say, its expectation value in any state, is the velocity of the particle! This seems very bizarre, but we will prove it conversely, by using the commutator of the position with the Hamiltonian,
\be
\frac{\dif}{\dif t}{\la x\ra}=\frac i \hbar \la[H,x]\ra=\frac i \hbar\la[\vec\al\cdot\vec p+\bet m, x]\ra=\frac i \hbar \la\al_x[p_x,x]\ra=\frac i \hbar \la\al_x\ra (-i\hbar)=\la\al_x\ra,
\ee
which holds not only for $x$-component, but also for $y$ and $z$. Thus we have
\be
\vec v=\frac{\dif}{\dif t}{\la \vec r\ra}=\la\vec\al\ra.
\ee
The velocity is the average of the $\al$ matrix, not the momentum like in classical mechanics! So if you want to know what the velocity is, you measure $\al$.

On the other hand, the $\al$ itself is not conserved. This is easy to be seen in the fact that the time derivative of $\al$ is proportional to the commutator of the Hamiltonian and the $\al$ as $[H,\al_x]$, since $H$ contains $\al_y$ and $\al_z$, this commutator would be non-zero. This means that the particle is accelerating. This does not violate the Newton's law, since firstly Newton's law says the time derivative of the momentum, not the derivative of the velocity, is zero; secondly the time average of the velocity over a certain period is indeed zero, which says the particle moves in a crazy manner in every instant, but in general it moves along a line.

Besides, since $\al$ is the velocity, it's vector quantity; it's not spin, which is an axial vector, or pseudo-vector quantity. We list the similar quantities as below
\be\begin{split}
\text{Velocity:}&\quad\quad \vec\al=\1(\ba{cc}\vec\sig&O\\O&-\vec\sig\ea\2)\\
\text{Spin:}&\quad\quad \vec\Sigma=\1(\ba{cc}\vec\sig&O\\O&\vec\sig\ea\2)\\
\text{Chirality:}&\quad\quad \vec\sig\cdot\vec p.
\end{split}\ee

\newpage
\section{Particle physics}
\be
\begin{gathered} \xymatrix@R=8ex@C=2ex{
\text{Particles} \ar@{<->}[rd] \ar@{<->}[rr]& &\text{Fields} \ar@{<->}[ld] \\
&\text{Forces}& \\
} \end{gathered}
\ee

\subsection{Spin multiplets and isospin}
If we put two half-spin particles together  to make a composite particle, what's the possible total spin and $z$-component of spin could it have?\\
Intuitively, there're four ways to align the spin of the two particles, with each particle spinning up or down along the $z$-axis, we pictorially denote them as
\be
|\uparrow\uparrow\ \ra,\quad |\downarrow\downarrow\ \ra,\quad |\uparrow\downarrow\ \ra,\quad |\downarrow\uparrow\ \ra.
\ee
We see that the maximum spin along the $z$-axis is $1$ when the two particle spins are parallel and pointing up, so there're some of the states which correspond to the total spin of $S=1$. But for $S=1$, there're only three possible values for $S_z$, namely $S_z=1$ corresponds to $|\uparrow\uparrow\ \ra$,$S_z=-1$ corresponds to $|\downarrow\downarrow\ \ra$, and $S_z=0$ corresponds to one of the left two or some combination of them, so there's one independent state left there which does not belong to this multiplets, what total spin does it belong to? The natural answer is $S=0$. \\
We give the $S=1$ multiplets as follows:
\be
|\uparrow\uparrow\ \ra,\quad |\downarrow\downarrow\ \ra,\quad \frac{1}{\sqrt{2}}\big(\ |\uparrow\downarrow\ \ra+ |\downarrow\uparrow\ \ra\ \big),
\ee
which are all symmetric under exchange of particles. And since they all correspond to the same spin  angular momentum,  they can be converted to each other by rotation. \\
The total spin $S=0$ state is given by
\be
\frac{1}{\sqrt{2}}\big(\ |\uparrow\downarrow\ \ra- |\downarrow\uparrow\ \ra\ \big),
\ee
it is anti-symmetric under exchange, and if we only consider the conservation of spin angular momentum, this $0$ spin state could have the possibility to decay or vanish.\\

Next we state the possibility of putting three half-spin particles together to make a composite, then this composite particle would have $2^3=8$ independent states, we may easily know  that $4$ of them correspond to the total spin $S=\frac 3 2$, and what about the left $4$ states? They correspond to $2$ distinct ways of making $S=\frac 1 2$ particles. We conclude this as
\be
\begin{gathered}
\xymatrix@R=3ex@C=1ex{
8=&4&+&2&\times& 2.\\
 &S=\frac 3 2\ar[u]&&S=\frac 1 2\ar[u]&& 2\ \text{ways of}\ ``S=\frac1 2"\ar[u]\\
}\end{gathered}
\ee

A concept in quark physics that is parallel to the ordinary spin is called the isospin, it's also a two-component quantity with each one called up --- ``u'', and down --- ``d". Here we state that the isospin is an approximate concept since we ignore the mass and the electric charge difference of the up and down quarks. The isospin of up and down quarks is given by $I=\frac1 2$, and we assign each one a ``third component of the isospin" with $I_3=\frac1 2$ to up quark and $I_3=-\frac1 2"$ to down quark, with their anti-quarks the opposite $I_3$. We denote the analog of isospin and ordinary spin as below
\be\begin{split}
(\uparrow, \downarrow) &\quad\longrightarrow\quad \text{spin}\\
(u, d) &\quad\longrightarrow\quad \text{isospin}
\end{split}\ee

In this sense, we can compose quark-antiquark pairs to form a meson only of the up and down and their antiquarks to give isospin multiplets. The $I=1$ multiplets are called the pions, the states of which are classified by their $I_3$ and charge as,
\be
\pi^+(I_3=1):|u\bar d\ra, \quad\pi^-(I_3=-1):|\bar u d\ra, \quad\pi^0(I_3=0):\frac1 {\sqrt{2}}\big(\ |u\bar u\ra-|d\bar d\ra\ \big),
\ee
with their mass close to each other, about $140$MeV. And the $I=0$ state is named as the $\eta$ particle,
\be
\eta(I_3=0): \frac1 {\sqrt{2}}\big(\ |u\bar u\ra+|d\bar d\ra\ \big),
\ee
with its mass a little heavier than the pions, about $500$MeV. By the way, the $\eta$ and the $\pi^0$ are entangled through the isospin.

Then we come to particles composed with three quarks which are called the hadrons. The most commonly seen hadrons are nucleons, i.e, protons and neutrons, they are made of up and down quarks and have the same ordinary spin $S=\frac1 2$ and isospin $I=\frac1 2$, we list them below
\be
p(I_3=\frac1 2): uud, \quad n(I_3=-\frac1 2)=udd,
\ee
they have the similar mass about $m_{p,n}\sim 940$ MeV.

Then the less commonly seen hadrons made up of three up or down quarks are isospin $I=\frac3 2$ particles, called $\Delta_{\frac3 2}$, they are listed by their electric charge $Q$ and the third component of isospin $I_3$ below,
\be\begin{split}
&uuu:\quad Q=2,\quad I_3=\frac3 2\\
&uud:\quad Q=1,\quad I_3=\frac1 2\\
&udd:\quad Q=0,\quad I_3=-\frac1 2\\
&ddd:\quad Q=-1,\quad I_3=-\frac3 2,
\end{split}\ee
the mass the $\Delta$ particles are about $m_\Delta\sim 1200$ MeV. By the way, the parameters like the mass that we describe particles with, are dependent on frequencies and wavelength of the interactions that engage in, thus they are given the name ``the running constant".

\subsec{Group theory of rotations}
We know from elementary physics that a vector in $3D$ is represented by a $3$-component vector, thus the rotation is represented by a $3\times 3$ matrix, then the resulting vector is given by
\be
v'_i=R_{ij}(\th,\hat n) V_j,
\ee
where  the $\hat n$ is the axis along which the vector rotates and the $\th$ is the rotation angle. Or this can be simply denoted by 
\be V'=RV. \ee
From physical experience we know that rotation reserves the length of the vector, which can give an important property of the rotation matrix which we will derive as follows:
\be\begin{split}
&V_jV_j=V_i'V_i'=R_{ij}V_jR_{ik}V_k=R_{ij}R_{ik}V_jV_k \quad\implies\quad R_{ij}R_{ik}=\del_{jk}  \\
&\quad\implies\quad R_{ji}^TR_{ik}=\del_{jk}\quad\implies\quad R^TR=I \quad\implies\quad R^T=R^{-1},
\end{split}\ee
where the implication of summation over repeated index is used, and this result says the $R$ matrix is an orthogonal matrix.

A particle which has $S=1$ has three spin $S_z$ components denoted the probability amplitude of $S_z=1,0,-1$, as $\1( \al_1, \al_2, \al_3\2)^T$, which just behaves like a ordinary vector. So if we want to rotate its direction of spin, we just rotate it by a $3\times3$ rotation matrix, in this sense we called a particle with $S=1$ a vector quantity.

Another kind of quantity is called the spinor quantity, it has $S=\frac1 2$ thus it is a $2$-component complex vector denoted as $(\al_1,\al_2)^T$. Now that it is complex  it has four free parameters, but not all of them are independent. One of the constraints is the fact that the total probability should be equal to one: $\al_1^*\al_1+\al_2^*\al_2=1$, while the second extra parameter is the phase factor which doesn't account for any physical difference. With these two conditions, the spinor quantity has only two independent parameters, which are as we expected consistent with the number of parameters needed to indicate a direction in space, say the longitude and the latitude.
If we want rotate the spinor we need a $2\times2$ complex matrix, which should again preserve the length, or the probability of the spinor, as $\al_1'^*\al_1'+\al_2'^*\al_2'=\al_1^*\al_1+\al_2^*\al_2=1$. This gives the similar property as that of the rotation matrix, which is not called the orthogonality, but unitarity, denoted as $U^\dag U=I$. Note that the unitary equation is a matrix equation thus it contains four constraints, each one for each matrix elements, thus we reduce the independent parameters of the rotation matrix to $8-4=4$. And yet there's an additional constraint which states that the determinant of the unitary matrix is $1$, $det(U)=1$, which implies that this is a ``genuine" rotation, not the inversion of it (det(U)=-1). In this sense, there're $4-1=3$ independent parameters for the rotation, which again coincides with our intuition that we need $2$ parameters to indicated the axis and $1$ parameter to indicate the angle that rotated around the axis. This kind of unitary rotation is called the $SU(2)$, where $S$ means ``Special", i.e, the determinant is one, and $U$ means unitarity, and $2$ means $2\times 2$ complex matrix. The $SU(2)$ symmetry group is a $2\times 2$ complex representation of the rotation  group, i.e, it's isomorphic to rotation in $3D$ space, while not every $SU(N)$ is like so. By the way, the interchange group is a subgroup of the rotation group, eg, $\1(\begin{array}{cc}0&i\\i&0\\\end{array}\2)$ interchange  the two components of a spinor, its determinant is one and it belongs to the $SU(2)$ group.\\

Next we build the unitary rotation by incremental changes and induce the generator, the process is much similar to those of  continuous time and space symmetry transformations. An infinitesimal unitary rotation is built up by
\be
U=I-i\ep T
\ee,
where $-i$ is due to convention and $\ep$ is an infinitesimal quantity, and $T$ is called the generator.\\
From the unitarity we may easily derive that the generator $T$ is Hermitian,
\be
U^\dag U=I \quad\implies\quad T^\dag=T,
\ee
which means that it's an observable, and actually it's the angular momentum in $2\times 2$ complex representation.\\
And further since the determinant of the unitary matrix is one, we may find another fact that $T$ is traceless,
\be
U=I-i\ep T=\1(\ba{cc}1-i\ep T_{11}&-iT_{12}\\-iT_{21}&i-iT_{22}\\\ea\2) \implies 1=det(U)=1-i\ep(T_{11}+T_{22})=1-i\ep\ tr(T) \implies tr(T)=0,
\ee
where we neglect the higher-order terms of $\ep$. In $2\times 2$ complex matrix space, there're $3$ independent traceless hermitian matrices, which are nothing but the Pauli matrices. Thus the generator can be built up by linear combinations of the Pauli matrices as
\be T=\sum_i a_i \sig_i. \ee

Then we come to the quark color symmetries. Quarks have three colors denoted by R, G and B, just like the ordinary spin, the color of a quark can be any linear combination of these three, thus it'll appear as a complex $3$-component vector, as $(\al_1,\al_2,\al_3)^T$. A rotation group in color space is called $SU(3)$ symmetry group, it's not an ordinary rotation isomorphic to rotation in $3D$ as $SU(2)$, but a new rotation that is hard to visualize. Just like $SU(2)$, there're two constraints for free parameters of an element of $SU(3)$, the unitarity and the unit determinant, thus they will reduce the number of independent parameters to $9\times 2-9-9=8$, corresponding to the mathematical structure of $8$ independent types of gluons. $SU(3)$ symmetry is implied in the Lagrangian of the color fields, i.e, every possible term in QCD Lagrangian should be invariant under $SU(3)$ transformation; this is parallel to every term in QED Lagrangian is invariant under $SU(1)$, which is there simply a phase factor and leads to the consequence that the number of fields and their complex conjugate fields must be the same.

\subsection{Gauge symmetry and covariant deriviatives}
If a complex changed scalar field $\phi$, and its complex conjugate transform as
\be\1\{\begin{split}
\phi &\rightarrow \Phi=e^{i\th(x)} \phi\\
\phi^* &\rightarrow \Phi=e^{-i\th(x)} \phi^*,
\end{split}\2.\ee
where $x$ represents for $4D$ spacetime coordinates. Then the ordinary derivative of the new fields are given by
\be\1\{\begin{split}
\p_\m\Phi&=\1(\p_\m\phi+i\phi\p_\m\th\2)e^{i\th(x)} \\
\p_\m\Phi^*&=\1(\p_\m\phi-i\phi\p_\m\th\2)e^{-i\th(x)},
\end{split}\2.\ee
thus the new inner product will be the old inner product plus some extra term, which is of course not invariant. Here we could introduce a gauge field (in this case the e-m field) and the covariant derivative to eliminate the extra terms. The transformations of the gauge field and the covariant derivative are given as follows
\be\1\{\begin{split}
A_\m&\rightarrow A_\m'=A_\m-\p_\m\th \\
\p_\m&\rightarrow D_\m =\p_\m+iA_\m,
\end{split}\2.\ee
and hence naturally the covariant derivative of the complex conjugate field is given by $D_\m\phi^*=\p_\m-iA_\n\th$. With this definition, we could calculate the covariant derivatives of the new field and its complex field as,
\be\1\{\begin{split}
D_\m \Phi&=\p_\m \Phi +iA'\Phi =\1(\p_\m\phi+i\phi\p_\m\th\2)e^{i\th(x)}+i\1(A_\m-\p_\m\th\2)\phi e^{i\th(x)}=\1(\p_\m\phi+iA_\m\phi\2)e^{i\th(x)}=D_\m\phi e^{i\th(x)}\\
D_\m \Phi^*&=\p_\m \Phi^* -iA'\Phi^* =....=D_\m\phi^* e^{-i\th(x)},
\end{split}\2.\ee
only differing from the original derivative by a phase, thus can be canceled in the inner product, explicitly as
\be
D_\m \Phi^*D_\m \Phi=D_\m \phi^*D_\m \phi.
\ee
Thus we reserve the gauge invariance of the kinetic term. And next the potential term is easy to be kept invariant only if it's a function of $\phi^*\phi$, so if the Lagrangian has the following form as
\be \ma L=D_\m \phi^*D_\m \phi-V(\phi^*\phi), \ee
it'll be invariant under the gauge transformation.\\
But we still have to add the gauge field term involving $F_{\m\n}=\p_\m A_\n-\p_\n A_\m$, note that it's itself gauge invariant as
\be F'_{\m\n}=\p_\m A'_\n-\p_\n A'_\m=\p_\m A_\n-\p_\m\p_\n\th-\p_\n A_\m+\p_\n\p_\m\th=\p_\m A_\n-\p_\n A_\m=F_{\m\n}, \ee
so the only thing to do is to make a scalar out of it as $F_{\m\n}F^{\m\n}$. Therefore we have a Lagrangian which has the local gauge symmetry, as
\be \ma L=D_\m \phi^*D_\m \phi-V(\phi^*\phi)+\frac 1 4 F_{\m\n}F^{\m\n}, \ee
where the factor of $\frac 1 4$ is due to convention (referring to classical field theory). Note that it's usually common to assign a mass to a field by a quadratic term of it, we may try to make a term as $\frac {m^2} 2 A^\m A_\m$ to make the gauge particle have the mass, but this term is itself not invariant under gauge transformation (also see the classical field theory), so it's not possible to add such a term; this is consistent with fact that photons don't have mass. But mass of a gauge field can be given by another mechanism known as the \textit{Spontaneous Symmetry Breaking}, as we will talk later.














\end{document}